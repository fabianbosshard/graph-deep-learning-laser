\documentclass{gdl}


\usepackage{graphicx}
\usepackage[dvipsnames]{xcolor}
\usepackage{soulutf8}
\usepackage{xparse}

\ExplSyntaxOn
\tl_new:N \__l_SOUL_argument_tl
\cs_set_eq:Nc \SOUL_start:n { SOUL@start }
\cs_generate_variant:Nn \SOUL_start:n { V }
\cs_set_protected:cpn {SOUL@start} #1
 {
  \tl_set:Nn \__l_SOUL_argument_tl { #1 }
  \regex_replace_all:nnN
   { \c{\(} (.*?) \c{\)} } % look for \(...\) (lazily)
   { \cM\$ \1 \cM\$ }      % replace with $...$
   \__l_SOUL_argument_tl
  \SOUL_start:V \__l_SOUL_argument_tl % do the usual
 }
\ExplSyntaxOff

% save soul's \hl under a private name
\let\SOULhl\hl
\renewcommand{\hl}[1]{\SOULhl{#1}} % keep plain \hl working if you want

% punchy highlighter colors
\definecolor{HLgreen}{HTML}{77DD77}
\definecolor{HLyellow}{HTML}{FFFF66}
\definecolor{HLorange}{HTML}{FFB347}
\definecolor{HLred}{HTML}{FF6961}
\definecolor{HLpink}{HTML}{FFB6C1}
\definecolor{HLturquoise}{HTML}{40E0D0}

% master highlight macro: \hl[level]{text}, default = green
\RenewDocumentCommand{\hl}{O{1} m}{%
  \begingroup
  \IfEqCase{#1}{%
    {1}{\sethlcolor{HLgreen}\SOULhl{#2}}%
    {2}{\sethlcolor{HLyellow}\SOULhl{#2}}%
    {3}{\sethlcolor{HLorange}\SOULhl{#2}}%
    {4}{\sethlcolor{HLred}\SOULhl{#2}}%
    {5}{\sethlcolor{red}\SOULhl{#2}}%
    {6}{\sethlcolor{HLturquoise}\SOULhl{#2}}%
  }[\PackageError{hl}{Undefined highlight level: #1}{}]%
  \endgroup
}










\usepackage{amsmath, amssymb, amsthm}
\usepackage{mathtools}

\usepackage{mathdots} % without this package, only \vdots and \ddots are taken from the text font (not the math font), which looks bad if the text font is different from the math font
\usepackage{nicematrix}


\usepackage{booktabs}
\usepackage{array}













\usepackage{pifont}


\usepackage{tikz}
\usetikzlibrary{arrows, arrows.meta, shapes, positioning, calc, fit, patterns, intersections, math, 3d, tikzmark, decorations.pathreplacing, decorations.markings, decorations.text}
\usepackage{tikz-3dplot}
\usepackage{tikzpagenodes}
% \usetikzlibrary{external}
% \tikzexternalize[prefix=tikz/]
\usepackage{pgfplots}


% define colors
\definecolor{funblue}{rgb}{0.10, 0.35, 0.66}
\definecolor{alizarincrimsonred}{rgb}{0.85, 0.17, 0.11}
\definecolor{amethyst}{rgb}{0.6, 0.4, 0.8}
\definecolor{mypurple}{rgb}{128, 0, 128} 
\definecolor{highlightpurple}{rgb}{0.6, 0.0, 0.6}
\renewcommand{\emph}[1]{\textcolor{highlightpurple}{#1}}

\usepackage{thmtools}

\newlength{\thmspace} \setlength{\thmspace}{3pt plus 1pt minus 1pt} 

\declaretheoremstyle[headfont=\bfseries, bodyfont=\normalfont, spaceabove=\thmspace, spacebelow=\thmspace;/, qed=\ensuremath{\vartriangleleft}, postheadspace=1em]{assertionstyle}
\declaretheorem[style=assertionstyle, name=Theorem]{theorem}
\declaretheorem[style=assertionstyle, name=Lemma, sibling=theorem]{lemma}
\declaretheorem[style=assertionstyle, name=Corollary, sibling=theorem]{corollary}
\declaretheorem[style=assertionstyle, name=Proposition, sibling=theorem]{proposition}
\declaretheorem[style=assertionstyle, name=Conjecture, sibling=theorem]{conjecture}
\declaretheorem[style=assertionstyle, name=Claim, sibling=theorem]{claim}
\declaretheorem[style=assertionstyle, name=Fact, sibling=theorem]{fact}

\declaretheoremstyle[headfont=\bfseries, bodyfont=\normalfont, spaceabove=\thmspace, spacebelow=\thmspace, qed=\ding{45}, postheadspace=1em]{definitionstyle}
\declaretheorem[style=definitionstyle, name=Definition]{definition}
\declaretheorem[style=definitionstyle, name=Problem, sibling=definition]{problem}


% \declaretheoremstyle[headfont=\bfseries, bodyfont=\normalfont, spaceabove=6pt, spacebelow=6pt, qed=\ensuremath{\blacktriangleleft}, postheadspace=1em]{definitionstyle}
% \declaretheorem[style=definitionstyle, name=Definition]{definition}

\declaretheoremstyle[headfont=\bfseries, bodyfont=\normalfont, spaceabove=6pt, spacebelow=6pt, qed=\ensuremath{\square}, postheadspace=1em]{proofstyle}
\let\proof\relax \let\endproof\relax
\declaretheorem[style=proofstyle, name=Proof, numbered=no]{proof}

\declaretheoremstyle[headfont=\bfseries\color{funblue}, bodyfont=\normalfont\normalsize, spaceabove=\thmspace, spacebelow=\thmspace, qed=\ensuremath{\color{funblue}\blacktriangleleft}, postheadspace=1em]{examplestyle}
\declaretheorem[style=examplestyle, name=Example, numbered=no]{example}

\declaretheoremstyle[headfont=\bfseries, bodyfont=\normalfont\normalsize, spaceabove=\thmspace, spacebelow=\thmspace, qed=\ensuremath{\blacktriangleleft}, postheadspace=1em]{remarkstyle}
\declaretheorem[style=remarkstyle, name=Remark, numbered=no]{remark}

\declaretheoremstyle[headfont=\color{alizarincrimsonred}\bfseries, bodyfont=\normalfont\normalsize, spaceabove=\thmspace, spacebelow=\thmspace, qed=\ensuremath{\color{alizarincrimsonred}\blacktriangleleft}, postheadspace=1em]{cautionstyle}
\declaretheorem[style=cautionstyle, name=Caution, sibling=remark, numbered=no]{caution}

\declaretheoremstyle[headfont=\bfseries, bodyfont=\normalfont\footnotesize, spaceabove=\thmspace, spacebelow=\thmspace, postheadspace=1em]{smallremarkstyle}
\declaretheorem[style=smallremarkstyle, name=Remark, sibling=remark, numbered=no]{smallremark}

\newenvironment{verticalhack}
  {\begin{array}[b]{@{}c@{}}\displaystyle}
  {\\\noalign{\hrule height0pt}\end{array}} % to make qed symbol aligned


\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage[italicComments=false]{algpseudocodex} % macht probleme mit TikZ externalization
\newcommand*{\algorithmautorefname}{Algorithm}


% commands for functions etc.
\newcommand{\im}{\operatorname{Im}}
\newcommand*\matrspace{0.8mu}
\newcommand{\matr}[1]{%
  \mspace{\matrspace}%
  \underline{%
    \mspace{-\matrspace}%
    \smash[b]{\boldsymbol{#1}}%
    \mspace{-\matrspace}%
  }%
  \mspace{\matrspace}%
}
\newcommand{\vect}[1]{\vec{\boldsymbol{#1}}}





% commands for derivatives
\newcommand{\dif}{\mathrm{d}}

% commands for number sets
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}

% commands for probability
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\Cov}{\operatorname{Cov}}
\newcommand{\Exp}{\operatorname{E}}
% \newcommand{\P}{\operatorname{P}} % this is already defined in amsmath/amsopn
\newcommand{\Prob}{\operatorname{P}}
\newcommand{\numof}{\ensuremath{\# \,}} % number of elements in a set
\newcommand{\blackheight}{\operatorname{bh}}

% \algnewcommand{\LeftComment}[1]{\(\triangleright\) #1}
\algnewcommand{\TO}{, \ldots ,}
\algnewcommand{\DOWNTO}{, \ldots ,}
\algnewcommand{\OR}{\vee}
\algnewcommand{\AND}{\wedge}
\algnewcommand{\NOT}{\neg}
\algnewcommand{\LEN}{\operatorname{len}}
\algnewcommand{\tru}{\ensuremath{\mathrm{\texttt{true}}}}
\algnewcommand{\fals}{\ensuremath{\mathrm{\texttt{false}}}}
\algnewcommand{\append}{\circ}

\algnewcommand{\nil}{\ensuremath{\mathrm{\textsc{nil}}}}
\algnewcommand{\red}{\ensuremath{\mathrm{\textsc{red}}}}
\algnewcommand{\black}{\ensuremath{\mathrm{\textsc{black}}}}
\algnewcommand{\gray}{\ensuremath{\mathrm{\textsc{gray}}}}
\algnewcommand{\white}{\ensuremath{\mathrm{\textsc{white}}}}

% \newcommand{\attribute}[1]{\ensuremath{\mathtt{#1}}}
\newcommand{\attrib}[2]{\ensuremath{#1\mathtt{.}\mathtt{#2}}} % object.field with field in math typewriter (like code)
\newcommand{\attribnormal}[2]{\ensuremath{#1\mathtt{.}#2}} 











%% commands from the paper
\newcommand{\gph}{G}
\newcommand{\MPNN}{\text{MPNN}}
\newcommand{\cheeg}{{h}_{\text{Cheeg}
}}
\newcommand{\res}{\text{Res}}
\newcommand{\V}{{V}}
\newcommand{\E}{{E}}
\newcommand{\GAMma}{\boldsymbol{\Gamma}}
%\newcommand{\MPNN}{\mathrm{MPNN}}
\newcommand{\Hi}{\mathbf{H}}
\newcommand{\Hit}{\mathbf{H}^{(t)}}
\newcommand{\up}{\texttt{up}}
\newcommand{\rs}{\text{r}}
\newcommand{\mpas}{\text{a}}
\newcommand{\agg}{\texttt{agg}}
\newcommand{\com}{\text{com}}
\newcommand{\colfirst}{\textcolor{red}}
\newcommand{\colsecond}{\textcolor{blue}}
\newcommand{\colthird}{\textcolor{violet}}
% \newcommand{\MPNN}{\text{MPNN}}
\newcommand{\eigen}{\boldsymbol{\psi}}
\newcommand{\cost}{{c}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\DELta}{\boldsymbol{\Delta}}
\newcommand{\KER}{\mathcal{K}}
\newcommand{\OMEga}{\boldsymbol{\Omega}}
\newcommand{\Anorm}{\boldsymbol{\text{A}}}
\newcommand{\tel}{\text{MPNN}_{\text{tel}}}
\newcommand{\poly}{\text{p}}
\newcommand{\ourname}{\text{telescopic}-\text{MPNN}}
\newcommand{\oper}{\boldsymbol{\text{S}}_{\rs,\mpas}}
\newcommand{\xb}{\vb{x}}
\newcommand{\Xb}{\mathbf{X}}
\newcommand{\Wb}{\vb{W}}
\newcommand{\Hb}{\mathbf{H}}
\newcommand{\Ab}{\mathbf{A}}
\newcommand{\Ib}{\mathbf{I}}
\newcommand{\hb}{\mathbf{h}}
\newcommand{\phib}{\boldsymbol{\phi}}
\newcommand{\pepfunc}{\texttt{Peptides-func}\xspace}
\newcommand{\pepstruct}{\texttt{Peptides-struct}\xspace}











% \usepackage[colorlinks=true, linkcolor=blue, urlcolor=blue, citecolor=green]{hyperref}
% \usepackage{hyperref} % for printing use this version (without colorlinks)
\usepackage[
  pdfauthor={Fabian Bosshard},
  pdftitle={USI - Locality-Aware Graph Rewiring in GNNs - Course Summary},
  pdfkeywords={USI, locality-aware graph rewiring in gnns, course summary, informatics},
  colorlinks=false,        % don't wrap links in a colour
  pdfborder={0 0 0}        % no border around links
]{hyperref}
\usepackage[
  type     = {CC},
  modifier = {by},
  version  = {4.0},
]{doclicense}
\usepackage{cleveref}



% load glossaries *after* hyperref
\usepackage[acronym,              % create an “acronym” glossary
            nomain,               % omit the main glossary (only acronyms)
            toc,                  % add list of acronyms to the ToC
            nonumberlist,         % omit page list in the printed glossary
            automake,
            nopostdot, nogroupskip, style=super, nonumberlist
           ]{glossaries-extra}


% choose how the first appearance looks:
\setabbreviationstyle[acronym]{long-short}

% must be issued once *after* loading glossaries
\makeglossaries


\newacronym{gdl}{GDL}{Graph Deep Learning}
\newacronym{gnn}{GNN}{Graph Neural Network}
\newacronym{gcn}{GCN}{Graph Convolutional Network}
\newacronym{gin}{GIN}{Graph Isomorphism Network}
\newacronym{gat}{GAT}{Graph Attention Network}
\newacronym{gso}{GSO}{Graph Shift Operator}
\newacronym{mpnn}{MPNN}{Message Passing Neural Network}
\newacronym{laser}{LASER}{Locality-Aware Sequential Rewiring}

% experimental details 
\newacronym{oom}{OOM}{Out Of Memory}
\newacronym{ap}{AP}{Average Precision}
\newacronym{mae}{MAE}{Mean Absolute Error}
\newacronym{mrr}{MRR}{Mean Reciprocal Rank}
\newacronym{lrgb}{LRGB}{Long Range Graph Benchmark}
\newacronym{sdrf}{SDRF}{Stochastic Discrete Ricci Flow}
\newacronym{borf}{BORF}{Batch Ollivier-Ricci Flow}
\newacronym{fosr}{FOSR}{First-Order Spectral Rewiring}
\newacronym{gtr}{GTR}{Greedy Total Resistance rewiring}







% CHANGE THESE
\def\groupid{FBFRJT} % Fabian Bosshard, Fanny Rorri, Jimena Tagle
\def\projectid{LASER}

\begin{document}

% CHANGE THIS
\title{Locality-Aware Graph Rewiring in GNNs}

% CHANGE THIS
\author{%
Fabian Bosshard, Fanny Rorri, Laura Jimena Tagle Muñoz \\
\texttt{\{\href{mailto:fabian.bosshard@usi.ch}{fabian.bosshard}, \href{mailto:fanny.rorri@usi.ch}{fanny.rorri}, \href{mailto:jimena.tagle@usi.ch}{jimena.tagle}\}@usi.ch}
}

\begin{abstract}
% Concise and self-contained description of your project, motivation and main findings.

% % Delete the following part before submitting the report
% \begin{center}
%     \sf\large\color{red} GENERAL NOTES
% \end{center}

% The report should be written as an article intended to present the findings of your work. Your aim should be to be clear and objective, substantiating your claims with references or empirical/theoretical evidence.
% We are well aware of the fact that carrying out machine learning experiments might be difficult and that often the final performance might be disappointing. For this reason, you will not be evaluated solely on quantitative aspect of your work, but mainly on the quality of your analysis and report.
% The length of the report should be between 4 and 8 pages (without considering references).
\end{abstract}

\maketitle


\tableofcontents


% table of acronyms (remember everything is in build folder)
\printglossary[type=\acronymtype, title={List of Acronyms}]




\section{Introduction}

% Here you should clarify the context of your project and the problem you are dealing with. 
% You should also make a brief summary of the main results and contributions (i.e., if you tried to replicate the results of an existing paper you should say if you were successful or not). 
% The introduction should help the reader to follow along for the rest of the paper.



\gls{laser}


\section{Background}

Let $G = (V, E)$ be an undirected graph with the
adjacency matrix $\matr{A} \in \R^{n\times n}$
\begin{equation}\label{eq:adjacency_matrix}
    \matr{A}_{uv}
    =
    \begin{cases}
        1 & (u,v) \in E\\
        0 & (u,v) \notin E
    \end{cases}
\end{equation}

The \emph{diagonal degree matrix} $\matr{D}\in\R^{n\times n}$ is defined by
\begin{equation}\label{eq:degree_matrix}
    \matr{D}_{uv}
    =
    \begin{cases}
        d_u &  u = v\\
        0 & u \ne v
    \end{cases}
\end{equation}
i.e. $\matr{D}$ simply places all node degrees on the diagonal.




\subsection{normalized adjacency and multi-hop propagation}


\begin{definition}\label{def:normalized_adjacency}
The \emph{symmetrically normalized adjacency matrix} is
\begin{equation}\label{eq:normalized_adjacency}
    {\color{Red}\boxed{\color{black}
    \hat{\matr{A}}
    =
    \matr{D}^{-1/2}\matr{A}\matr{D}^{-1/2}
    }}
\end{equation}
or, entrywise,
\[
\begin{verticalhack}
    \hat{\matr{A}}_{uv}
    =
    \begin{cases}
        \dfrac{1}{\sqrt{d_u d_v}} & (u,v) \in E\\[0.5ex]
        0 & (u,v) \notin E
    \end{cases}
    \end{verticalhack}
    \qedhere
\]
\end{definition}

\begin{fact}[multi-hop propagation]\label{prop:multi_hop_propagation}
The entry $(\hat{\matr{A}}^{k})_{vu}$ can be computed explicitly as follows:
\begin{equation}\label{eq:multi_hop_propagation}
    (\hat{\matr{A}}^{k})_{vu}
    =
    \sum_{\pi}
    \prod_{(x, y) \in E_\pi} \frac{1}{\sqrt{d_x d_y}}
\end{equation}
the sum is over all walks $\pi = (v, \ldots, u)$ of length $k$ from $v$ to $u$ and the product is over the edges \(E_\pi = \{(v, u_1), \ldots, (u_{k-1}, u)\}\) on the walk.
\end{fact}



\begin{corollary}\label{cor:multi_hop_propagation_single_path}
Let $v,u \in V$ with $r = d_{G}(v,u)$, where $d_{G}(\cdot,\cdot)$ denotes the shortest-path distance.
Assume there is exactly one path
\[
    (v, u_{1}, \ldots, u_{r-1}, u)
\]
of length $r$ between $v$ and $u$:
\[
\begin{tikzpicture}[
  font=\footnotesize,
  scale=0.4,
  vertex/.style={
    circle,
    draw,
    fill=green!40,
    minimum size=10pt,
    inner sep=0,
    label={
        [anchor=base, label distance=5pt]
        below:{#1} % parentheses necessary, otherwise the '=' breaks TikZ parsing
    }
  },
  measure/.style={|-|}
]

\node[vertex={$v$}] (v) at (0, 0) {};
\node[vertex={$u_{1}$}] (u1) at (3, 0) {};
\node[vertex={$u_{r-1}$}] (urm1) at (9, 0) {};
\node[vertex={$u$}] (u) at (12, 0) {};


\draw[] (v) -- (u1);
\draw[] (u1) -- ($(u1)!1/3!(urm1)$);
\draw[dotted] ($(u1)!1/3!(urm1)$) -- ($(u1)!2/3!(urm1)$);
\draw[] ($(u1)!2/3!(urm1)$) -- (urm1);
\draw[] (urm1) -- (u);


\def\offset{-0.5cm};
\draw[measure] ($(v) + (0, -1) + (0, \offset)$) -- node[below, outer sep = 2pt, inner sep = 0] {$d_{G}(v,u) = r$} ($(u) + (0, -1) + (0, \offset)$);


\end{tikzpicture}
\]
Then
\begin{equation}\label{eq:multi_hop_propagation_single_path}
    \begin{aligned}
    (\hat{\matr{A}}^{r})_{vu}
    &=
    \frac{1}{\sqrt{d_v d_{u_{1}}}} \cdot
    \prod_{i=1}^{r-2} \frac{1}{\sqrt{d_{u_{i}} d_{u_{i+1}}}} \cdot
    \frac{1}{\sqrt{d_{u_{r-1}} d_{u}}} \\
    &=
    \frac{1}{\sqrt{d_v d_u}} \prod_{i=1}^{r-1} \frac{1}{d_{u_i}}
    \end{aligned}
\end{equation}
\end{corollary}



\subsubsection{distance layers and layer degrees}

% For \gls{laser} and related methods it is useful to separate connections by graph distance.

\begin{definition}\label{def:distance_layer_adjacency}
For $\ell \in \N_0$, we define the \emph{distance-($\ell+1$) adjacency matrix} $\matr{A}_{\ell} \in \R^{n\times n}$ by
\begin{equation}\label{eq:distance_layer_adjacency}
    \bigl(\matr{A}_{\ell}\bigr)_{uv}
    =
    \begin{cases}
        1 & d_{G}(u,v) = \ell+1\\
        0 & \text{otherwise}
    \end{cases}
\end{equation}
where $d_{G}(u,v)$ is the shortest-path distance.
The corresponding \emph{layer degree} of a node $v$ at distance level $\ell$ is
\begin{equation}\label{eq:layer_degree}
    d_{v,\ell}
    =
    \sum_{u \in V} \bigl(\matr{A}_{\ell}\bigr)_{vu},
\end{equation}
i.e.\ the number of nodes at graph distance $\ell+1$ from $v$.
Let $\matr{D}_{\ell}$ be the diagonal matrix with $(\matr{D}_{\ell})_{vv} = d_{v,\ell}$.
The \emph{normalized distance-$(\ell+1)$ adjacency} is
\begin{equation}\label{eq:normalized_distance_layer_adjacency}
    {\color{Red}\boxed{\color{black}
    \hat{\matr{A}}_{\ell}
    =
    \matr{D}_{\ell}^{-1/2} \matr{A}_{\ell} \matr{D}_{\ell}^{-1/2}
    }}
\end{equation}
so that
\[
\begin{verticalhack}
\bigl(\hat{\matr{A}}_{\ell}\bigr)_{uv}
=
\begin{cases}
    \frac{1}{\sqrt{d_{u,\ell} d_{v,\ell}}} & d_{G}(u,v) = \ell+1 \\
    0 & \text{otherwise}
\end{cases}
\end{verticalhack}
\qedhere
\]
\end{definition}

% Finally, we denote by
% \begin{equation}\label{eq:minimum_degree}
%     d_{\min}
%     =
%     \min_{v \in V} d_v
% \end{equation}
% the \emph{minimum node degree} in the graph.







\subsection{graph Laplacian}

\begin{definition}\label{def:graph_laplacian}
The \emph{combinatorial graph Laplacian} is
\begin{equation}\label{eq:graph_laplacian}
    \matr{L} = \matr{D} - \matr{A}
\end{equation}
and the \emph{normalized graph Laplacian} is
\begin{equation}\label{eq:normalized_graph_laplacian}
    {\color{Red}\boxed{\color{black}
    \hat{\matr{L}}
    =
    \matr{D}^{-1/2}\matr{L}\matr{D}^{-1/2}
    \overset{\text{\eqref{eq:graph_laplacian}}}{=}
    \matr{D}^{-1/2}(\matr{D} - \matr{A})\matr{D}^{-1/2}
    \overset{\text{\eqref{eq:normalized_adjacency}}}{=}
    \matr{I}_{n} - \hat{\matr{A}}
    }}
\end{equation}
It is symmetric and positive semidefinite, and its eigenvalues satisfy
\[
    0 = \lambda_{0} \le \lambda_{1} \le \dots \le \lambda_{n-1}
\]
$\lambda_1$ is called the \emph{spectral gap}.
The number of zero eigenvalues (i.e., the multiplicity of the \(0\) eigenvalue) equals the number of connected components of the graph.
\end{definition}

To understand \autoref{def:graph_laplacian}, consider a function $f\colon V \to \R$.
Denote by $\vect{f} \in \R^{n}$ the vector whose $v$-th entry is $f(v)$.
Then
% \begin{equation}\label{eq:laplacian_action_on_function}
% (\matr{L}\vect{f})_v
% =
% d_v f(v) - \sum_{(u,v) \in E} f(u)
% \end{equation}
% i.e., $(\matr{L}\vect{f})_v$ is (up to the factor $d_v$) the difference between the value at $v$ and the sum of the values at its neighbors.
% For the normalized Laplacian, we have
\begin{equation}\label{eq:normalized_laplacian_action_on_function}
(\hat{\matr{L}}\vect{f})_v
=
f(v)
-
\frac{1}{\sqrt{d_v}}
\sum_{(u,v) \in E}
\frac{f(u)}{\sqrt{d_u}}
\end{equation}
i.e., $(\hat{\matr{L}}\vect{f})_v$ is the value at $v$ minus a degree-normalized average of the neighbors.
This is why the Laplacian is often viewed as a \emph{discrete second derivative} on the graph:
\hl[2]{it measures how much $f$ at $v$ deviates from its neighborhood}.
Another important identity is the quadratic form
\begin{equation}\label{eq:laplacian_quadratic_form}
    \vect{f}^{\top}\matr{L}\vect{f}
    =
    \frac{1}{2}
    \sum_{(u,v) \in E}
    \bigl(f(u) - f(v)\bigr)^{2}
\end{equation}
which shows that \(\matr{L}\) (and hence also \(\hat{\matr{L}}\)) is positive semidefinite, since the right-hand side is always nonnegative.
Moreover, \eqref{eq:laplacian_quadratic_form} is small exactly when $f$ varies slowly across edges, so the Laplacian encodes the \emph{smoothness} of functions on the graph.




\subsection{Cheeger inequality}

The \emph{Cheeger inequality} relates the spectral gap $\lambda_1$ to the \emph{Cheeger constant} $h(G)$, which measures how difficult it is to separate the graph into two large pieces.  It states, in particular, that
\[
    \tfrac{1}{2}h(G)^2
    \le
    \lambda_1
    \le
    2 h(G),
\]
so a larger spectral gap implies that the graph is more ``well-connected''.


\subsection{effective resistance}
\begin{definition}[effective resistance]\label{def:effective_resistance}
View each edge $(u,v)\in E$ as an electrical resistor of resistance $1\,$\(\Omega\).
The resulting network has a well-defined resistance between any two nodes.

For two nodes $s,t \in V$, the \emph{effective resistance} $R(s,t)$ is defined as the voltage difference needed to send one unit of electrical current from $s$ to $t$.
It can be computed as
\begin{equation}\label{eq:effective_resistance}
{\color{Red}\boxed{\color{black}
    R(s,t)
    =
    (\vect{e}_s - \vect{e}_t)^{\top}
    \matr{L}^{\dagger}
    (\vect{e}_s - \vect{e}_t)
}}
\end{equation}
where $\matr{L}^{\dagger}$ is the Moore--Penrose pseudoinverse of $\matr{L}$ and $\vect{e}_v$ is the standard basis vector of vertex $v$.
\end{definition}


\subsubsection{Interpretation}
If the graph offers many short, parallel paths between $s$ and $t$, then current can flow easily, so $R(s,t)$ is small.
If there are few or long paths, the current is ``bottlenecked'' and $R(s,t)$ is large.
Thus, effective resistance measures how ``well-connected'' two nodes are inside the global geometry of the graph.

\subsubsection{Connection to random walks}
A \emph{random walk} on $G$ is the Markov chain that, from a node $v$, moves to a uniformly random neighbor of $v$.  Its transition matrix is
\begin{equation}\label{eq:random_walk_transition_matrix}
\matr{P} = \matr{D}^{-1}\matr{A}
\end{equation}
so $\matr{P}_{vu} = 1/d_v$ if $(v,u)\in E$.

\medskip
For two nodes $u,v$, the \emph{commute time} $\texttt{CT}(u,v)$ is the expected number of steps for the random walk to start at $u$, reach $v$, and return to $u$ again.  
It can be related to the \nameref{def:effective_resistance} via
\begin{equation}\label{eq:commute_time_effective_resistance}
    \texttt{CT}(u,v)
    =
    2|E|
    R(u,v)
\end{equation}
giving a geometric interpretation of how ``far apart'' two nodes are in terms of random-walk behavior,
i.e. two nodes have small commute time exactly when they have small effective resistance.


\section{Related works}

% Give a brief summary of (some) existing methods that are related to you project. For instance, you can refer to~\citet{gilmer2017neural}, or simply~\cite{gilmer2017neural}, for introducing Message Passing Neural Networks. In this section it is important to provide readers references to the current state of the art and the foundations of the presented method. 

% N.B.: When referencing a different approach, it is not necessary to provide a detailed description, only one/two brief sentences are enough. The interested readers can eventually read the referenced work. 







\section{Theoretical analysis}\label{sec:theoretical_analysis}


\subsection{Dynamic rewiring paradigm}

The main contribution of \cite{barbero2024laser} is the following idea:
Instead of applying a single rewired graph \(\mathcal{R}(G)\), consider a sequence of rewired graphs
\begin{equation}\label{eq:sequence_rewiring}
\gph = \gph_0 \xhookrightarrow[]{\mathcal{R}} \cdots \xhookrightarrow[]{\mathcal{R}} \gph_{\ell-1} \xhookrightarrow[]{\mathcal{R}} \gph_{\ell} \xhookrightarrow[]{\mathcal{R}} \cdots \xhookrightarrow[]{\mathcal{R}} \gph_L
\end{equation}
where \(G_\ell\) is called the \(\ell\)-snapshot, since we think of \(G_\ell\) as the input graph evolved along a dynamical process for \(\ell\) iterations.


The variant to derive the theoretical results in \cite{barbero2024laser} is a \gls{gcn}-\gls{laser} architecture
\begin{equation}\label{eq:laser-GCN}
\begin{aligned}
% \vect{x}_{v}^{(t)} 
%  =&  \operatorname{ReLU}\Bigg( 
%   \sum_{u\in \mathcal{N}_1(v)\cup\{v\}}\frac{\matr{W}_0^{(t)}\vect{x}_{u}^{(t-1)} }{\sqrt{d_v d_u}}\\
%  &+ 
%  \sum_{\ell = 1}^{L} \sum_{u\in\mathcal{N}^\rho_{\ell+1}(v)}
% \frac{\matr{W}_{\ell}^{(t)} \vect{x}_u^{(t-1)}}{\sqrt{d_{v,\ell} d_{u,\ell}}}
% \Bigg)
\vect{x}_{v}^{(t)} 
 =&  \operatorname{ReLU}\Bigg( 
  \sum_{u\in \mathcal{N}_1(v)\cup\{v\}}
  \frac{1}{\sqrt{d_v d_u}} \cdot \matr{W}_0^{(t)}\vect{x}_{u}^{(t-1)}\\
 &+ 
 \sum_{\ell = 1}^{L} 
 \sum_{u\in\mathcal{N}^\rho_{\ell+1}(v)}
\frac{1}{\sqrt{d_{v,\ell} d_{u,\ell}}} \cdot \matr{W}_{\ell}^{(t)} \vect{x}_u^{(t-1)}
\Bigg)
\end{aligned}
\end{equation}
where \(\matr{W}_{\ell}^{(t)}\), for \(\ell = 0, \ldots, L\), 
are learnable weight matrices and 
\begin{equation}\label{eq:laser-neighborhood}
\mathcal{N}^\rho_{\ell + 1}(v) \subset \mathcal{N}_{\ell + 1}(v) := \{u \in V \mid d_{G_0}(u,v) = \ell + 1\}
\end{equation}
is a \(\rho\)-fraction of nodes at distance exactly \(\ell + 1\) from \(v\) with the lowest connectivity score \(\mu\) in the original graph \(G = G_0\). 
So, for the \(\ell^{\text{th}}\) snapshot \(G_\ell\), \gls{laser} considers \(\rho \cdot d_{v,\ell}\) artificial edges from \(v\) to nodes at distance \(d_{G_0}(u,v) = \ell + 1\).




\subsection{Jacobian sensitivity}

Let \(\matr{J}^{(t)}(v, u) := \partial \vect{x}_v^{(t)} / \partial \vect{x}_u^{(0)}\) be the Jacobian of features after \(t\) layers of message passing on \(G\) using a \gls{gcn} as described in \cite{kipf2016semi},
and similarly let \(\smash{\tilde{\matr{J}}}^{(t)}(v, u)\) be the Jacobian after \(t\) layers using \gls{laser}, as defined in \autoref{eq:laser-GCN}.

\begin{proposition}\label{prop:jacobian_sensitivity}
Let $v,u \in V$ with $r = d_{G_0}(v,u)$, where $G_0$ is the original graph before applying \gls{laser}.
Assume there is exactly one path \(\pi \subset G_0\) of length \(r\) connecting \(v\) and \(u\) (see \autoref{fig:jacobian_sensitivity_path}).


\begin{figure}[h]
\centering
\begin{tikzpicture}[
  font=\footnotesize,
  scale=0.45,
  vertex/.style={
    circle,
    draw,
    fill=green!40,
    minimum size=10pt,
    inner sep=0,
    label={
        [anchor=base, label distance=5pt]
        below:{#1} % parentheses necessary, otherwise the '=' breaks TikZ parsing
    }
  },
  measure/.style={
    |-|,
    >=stealth
  }
]

\node[vertex={$v = u_0$}] (v) at (0, 0) {};
\node[vertex={$u_{\ell-1}$}] (ulm1) at (6, 0) {};
\node[vertex={$j = u_\ell$}] (j) at (9, 0) {};
\node[vertex={$u_{\ell+1}$}] (ulp1) at (12, 0) {};
\node[vertex={$u = u_r$}] (u) at (18, 0) {};

\draw[] (v) -- ($(v)!1/3!(ulm1)$);
\draw[dotted] ($(v)!1/3!(ulm1)$) -- ($(v)!2/3!(ulm1)$);
\draw[] ($(v)!2/3!(ulm1)$) -- (ulm1);

\draw[] (ulm1) -- (j);
\draw[] (j) -- (ulp1);

\draw[] (ulp1) -- ($(ulp1)!1/3!(u)$);
\draw[dotted] ($(ulp1)!1/3!(u)$) -- ($(ulp1)!2/3!(u)$);
\draw[] ($(ulp1)!2/3!(u)$) -- (u);


\draw[blue, densely dashed] (v) to[out=45, in=135] node[pos=0.24, above, rotate=25] {LASER} (j);
\node[above right=0pt of j, rotate=45, outer sep=2pt, inner sep=0pt, anchor=west] {$\in \mathcal{N}_{\ell}^{\rho}(v)$};

\def\offset{-0.5cm};
\draw[measure] ($(v) + (0, -1) + (0, \offset)$) -- node[below, outer sep = 2pt, inner sep = 0] {$
% d_{G_0}(v,u) = 
r$} ($(u) + (0, -1) + (0, \offset)$);
\draw[measure] ($(v) + (0, -2) + (0, \offset)$) -- node[below, outer sep = 2pt, inner sep = 0] {$
% d_{G_0}(v,j) = 
\ell$} ($(j) + (0, -2) + (0, \offset)$);
\draw[measure, densely dashed] ($(v) + (0, -3) + (0, \offset)$) -- node[below, outer sep = 2pt, inner sep = 0] {$
% d_{G_{\color{red} \ell - 1}}(v,u) = 
\color{blue} 1 + \color{black} r - \ell$} ($(u) + (0, -3) + (0, \offset)$);

\end{tikzpicture}
\caption{
Unique shortest path \(\pi \subset G_0\) between \(v\) and \(u\) of length \(r\) with an intermediate node \(j\) at distance \(\ell\) from \(v\).
The dashed blue line indicates the edge added by \gls{laser}.
}
\label{fig:jacobian_sensitivity_path}
\end{figure}


\noindent
Assume \gls{laser} adds an edge between \(v\) and a node \(j \in V_\pi\) with \(d_{G_0}(v,j) = \ell < r\).
Furthermore, assume that all products of weight matrices have unit norm.
Then for all \(m \leq r\) we have
\begin{equation}\label{eq:jacobian_sensitivity}
\left\|\Exp\left[\smash{\tilde{\matr{J}}}^{(r - \ell + 1)}(v, u)\right]\right\|
\geq
\frac{(d_{\min})^{\ell} \sqrt{\frac{d_j}{d_u}}}{\sqrt{d_{v,\ell - 1} d_{j,\ell - 1}}}
\cdot
\left\|\Exp\left[\matr{J}^{(m)}(v, u)\right]\right\|
\end{equation}
where \(d_{\min} = \min_{v \in V} d_v\) is the minimum degree in \(G_0\).
\end{proposition}


\begin{proof}
In the following, the expectations are with respect to \(\operatorname{ReLU}'\) which is assumed to have probability of success \(p\) for all paths in the computational graph as in \cite{xu2018representation, digiovanni2023over}.
The Jacobian after \(m\) layers of \gls{gcn} is
\[
\left\| \Exp\left[\matr{J}^{(m)}(v, u)\right] \right\|
=
 p \left\| \prod_{k=1}^{m} \matr{W}^{(k)} \right\| (\hat{\matr{A}}^{m})_{vu}
 =
  p (\hat{\matr{A}}^{m})_{vu}
\]
where we used the unit-norm assumption on the weight matrices and \(\hat{\matr{A}}\) denotes the normalized adjacency matrix from \autoref{def:normalized_adjacency}.

For \(m < r\), the expression above vanishes, satisfying \eqref{eq:jacobian_sensitivity}, so we consider \(m = r\) from now on.

With the same assumptions, the Jacobian after \(r - \ell + 1\) layers of \gls{laser} is
\[
\begin{aligned}
\left\| \Exp\left[\smash{\tilde{\matr{J}}}^{(r - \ell + 1)}(v, u)\right] \right\|
=&
  p \left\| \prod_{k=2}^{r - \ell + 1} \matr{W}_0^{(k)}  {\color{blue}\matr{W}_{\ell - 1}^{(1)}} \right\|
  (\hat{\matr{A}}_{\ell - 1})_{vj} (\hat{\matr{A}}^{r - \ell})_{ju} \\
=&
  p (\hat{\matr{A}}_{\ell - 1})_{vj} (\hat{\matr{A}}^{r - \ell})_{ju}
\end{aligned}
\]

As we can see in \autoref{fig:jacobian_sensitivity_path}, we have a unique path
\[
(v = u_0, u_1, \dots, u_{r-1}, u_r = u)
\]
and we assume \(j = u_\ell\).
From \autoref{cor:multi_hop_propagation_single_path}, for the full path from \(v\) to \(u\):
\[
(\hat{\matr{A}}^{r})_{vu}
=
\frac{1}{\sqrt{d_v d_u}}
\prod_{s=1}^{r-1} \frac{1}{d_{u_s}}
\]

For the sub-path from \(j = u_\ell\) to \(u = u_r\) of length \(r-\ell\), the same reasoning gives
\[
(\hat{\matr{A}}^{r-\ell})_{ju}
=
\frac{1}{\sqrt{d_j d_u}}
\prod_{s=\ell+1}^{r-1} \frac{1}{d_{u_s}}.
\]

Now we calculate the ratio
\[
\begin{aligned}
\frac{
\left\| \Exp\left[\smash{\tilde{\matr{J}}}^{(r - \ell + 1)}(v, u)\right] \right\|
}{
\left\| \Exp\left[\matr{J}^{(r)}(v, u)\right] \right\|
}
&=
\frac{(\hat{\matr{A}}_{\ell-1})_{vj}(\hat{\matr{A}}^{r-\ell})_{ju}}{\frac{1}{\sqrt{d_v d_u}}\prod_{s = 1}^{r-1}\frac{1}{d_{u_s}}} \\
&=
\frac{(\hat{\matr{A}}_{\ell-1})_{vj}}{\frac{1}{\sqrt{d_v d_u}}\prod_{s = 1}^{\color{red}\ell\color{black}-1}\frac{1}{d_{u_s}}}
\cdot
\frac{(\hat{\matr{A}}^{r-\ell})_{ju}}{\prod_{s = {\color{red}\ell}}^{r-1}\frac{1}{d_{u_s}}} \\
&\overset{\text{\eqref{eq:multi_hop_propagation_single_path}}}{=}
\color{gray}
\frac{(\hat{\matr{A}}_{\ell-1})_{vj}}{\frac{1}{\sqrt{d_v d_u}}\prod_{s = 1}^{\ell-1}\frac{1}{d_{u_s}}}
\color{black}
\cdot
\frac{\frac{1}{\sqrt{d_j d_u}} \prod_{s=\ell+1}^{r-1} \frac{1}{d_{u_s}}}
{\prod_{s = \ell}^{r-1}\frac{1}{d_{u_s}}} \\
&=
\color{gray}
\frac{(\hat{\matr{A}}_{\ell-1})_{vj}}{\frac{1}{\sqrt{d_v d_u}}\prod_{s = 1}^{\ell-1}\frac{1}{d_{u_s}}}
\color{black}
\cdot
\frac{\frac{1}{\sqrt{d_j d_u}}\prod_{s=\ell+1}^{r-1} \frac{1}{d_{u_s}}}{\frac{1}{d_{u_\ell}}\prod_{s=\ell+1}^{r-1} \frac{1}{d_{u_s}}} \\
&=
\color{gray}
\frac{(\hat{\matr{A}}_{\ell-1})_{vj}}{\frac{1}{\sqrt{d_v d_u}}\prod_{s = 1}^{\ell-1}\frac{1}{d_{u_s}}}
\color{black}
\cdot
\frac{d_{u_\ell}}{\sqrt{d_j d_u}} \\
&=
% \color{gray}
\frac{(\hat{\matr{A}}_{\ell-1})_{vj}}{\frac{1}{\sqrt{d_v d_u}}\prod_{s = 1}^{\ell-1}\frac{1}{d_{u_s}}}
% \color{black}
\cdot
\sqrt{\frac{d_j}{d_u}}
\end{aligned}
\]
% So
% \begin{equation}\label{eq:laser_ratio_exact}
% \frac{(\hat{\matr{A}}_{\ell-1})_{vj}(\hat{\matr{A}}^{r-\ell})_{ju}}
% {\frac{1}{\sqrt{d_v d_u}}\prod_{s = 1}^{r-1}\frac{1}{d_{u_s}}}
% =
% \frac{(\hat{\matr{A}}_{\ell-1})_{vj}}{\frac{1}{\sqrt{d_v d_u}}\prod_{s = 1}^{\ell-1}\frac{1}{d_{u_s}}}
% \cdot
% \sqrt{\frac{d_j}{d_u}}
% \end{equation}
Using
\[
(\hat{\matr{A}}_{\ell-1})_{vj} 
\overset{\text{\eqref{eq:normalized_distance_layer_adjacency}}}{=} 
\frac{1}{\sqrt{d_{v,\ell-1} d_{j,\ell-1}}}
\]
and
\[
\frac{1}{\sqrt{d_v d_u}}\prod_{s=1}^{\ell-1} \frac{1}{d_{u_s}}
\le
\frac{1}{(d_{\min})^\ell}
\]
we obtain
\[
\frac{(\hat{\matr{A}}_{\ell-1})_{vj}}{\frac{1}{\sqrt{d_v d_u}}\prod_{s = 1}^{\ell-1}\frac{1}{d_{u_s}}}
\ge
\frac{(d_{\min})^\ell}{\sqrt{d_{v,\ell-1} d_{j,\ell-1}}}
\]
which, combined with the previous equation, gives
\[
\frac{
\left\| \Exp\left[\smash{\tilde{\matr{J}}}^{(r - \ell + 1)}(v, u)\right] \right\|
}{
\left\| \Exp\left[\matr{J}^{(r)}(v, u)\right] \right\|
}
\ge
\frac{(d_{\min})^\ell}{\sqrt{d_{v,\ell-1} d_{j,\ell-1}}}
\cdot
\sqrt{\frac{d_j}{d_u}}
\]
which can be rearranged ro the claimed bound \eqref{eq:jacobian_sensitivity}.
\end{proof}
\begin{remark}
Compared to the bound stated in \cite{barbero2024laser}, our derivation yields an additional multiplicative factor $\sqrt{d_j/d_u}$ in~\eqref{eq:jacobian_sensitivity}, 
which arises from the explicit evaluation of the subpath contribution $(\smash{\hat{\matr{A}}}^{r-\ell})_{ju}$. 
This factor is generally non-unit unless further degree assumptions are imposed, 
but does not affect the qualitative conclusion on the benefit of \gls{laser} for alleviating over-squashing.
\end{remark}




\subsection{Preservation of locality}



\begin{proposition}\label{prop:locality_preservation}
Let $\gph$ be a lollipop graph as in \autoref{fig:lollipop_graph}, composed of a chain of length $L$ attached to a clique of size $n$.
Consider a spectral rewiring $\mathcal{R}$ that adds a single edge between the pair of nodes with largest effective resistance.
Then there exists a choice of $\rho = \rho(L) \in (0,1)$ such that \gls{laser} with a single snapshot satisfies
\begin{equation}\label{eq:locality_preservation_goal}
  \|\mathcal{D}_\gph - \mathcal{D}_{\mathcal{R}(\gph)} \|_{F} 
  \geq
  \|\mathcal{D}_\gph - \mathcal{D}_{\text{\gls{laser}}} \|_{F}
\end{equation}
for all sufficiently large $n$.
\end{proposition}






\begin{figure}[h]
\centering
\begin{tikzpicture}[
  font=\footnotesize,
  scale=0.5,
  vertex/.style={
    circle,
    draw,
    fill=green!40,
    minimum size=10pt,
    inner sep=0
  },
  measure/.style={
    |-|,
    >=stealth
  }
]

\def\zLocation{10cm}
\def\cliqueRadius{2.5cm}
\coordinate (cliqueCenter) at ($(\zLocation,0) + (\cliqueRadius,0)$);

\node[circle, draw, dashed, fill=orange!20, minimum size=\cliqueRadius, inner sep=0pt] (clique) at (cliqueCenter) {};
\node at ($(cliqueCenter) + (0, 1/3*\cliqueRadius)$) {$|\texttt{clique}| = n$};


\node[vertex, label={[anchor=base, label distance=7pt]below:{$v$}}] (v) at (0, 0) {};
\node[vertex, label={[anchor=base, label distance=7pt]below:{$z'$}}] (zprime) at ($(\zLocation,0) - (0.2*\zLocation,0)$) {};
\node[vertex, label={[anchor=base, label distance=7pt, xshift=-6pt, yshift=3]below:{$z$}}] (z) at (\zLocation, 0) {};

\node[vertex, label={[anchor=base, label distance=7pt, xshift=6pt, yshift=3]below:{$u$}}] (cliqueNode1) at ($(cliqueCenter) - (0, 1/3*\cliqueRadius)$) {};


\draw[] (v) -- ($(v)!1/5!(zprime)$);
\draw[dotted] ($(v)!1/5!(zprime)$) -- ($(v)!4/5!(zprime)$);
\draw[] ($(v)!4/5!(zprime)$) -- (zprime);

\draw[] (zprime) -- (z);

\def\offset{-0.5cm};
\draw[measure] ($(v) + (0, -1) + (0, \offset)$) -- node[below, outer sep = 2pt, inner sep = 0] {$L$} ($(z) + (0, -1) + (0, \offset)$);
\end{tikzpicture}
\caption{
Lollipop graph with a chain of length \(L\) attached to a clique of size \(n\), a popular worst-case scenario for over-squashing \cite{digiovanni2023over,black2023understanding}.
}
\label{fig:lollipop_graph}
\end{figure}





\begin{proof}
Let $v$ be the endpoint of the chain, $z$ the articulation node (shared by chain and clique), and $u$ any node in the interior of the clique.
It is known that the commute time~\eqref{eq:commute_time_effective_resistance} between $v$ and $u$ scales cubically in the total number of nodes \citep{chandra1996electrical}, so a rewiring that minimizes effective resistance will add an edge between $v$ and some interior clique node, which we denote again by $u$.


Before rewiring, $d_\gph(v,z) = L$ and $d_\gph(v,u) = L+1$.
After adding the edge $(v,u)$, we have $d_{\mathcal{R}(\gph)}(v,u)=1$ and $d_{\mathcal{R}(\gph)}(v,z)\le 2$.
For every interior clique node $u'$ (there are $n-1$ of them, including $u$), 
\(
d_\gph(v,u') \ge L+1
\),
\(
d_{\mathcal{R}(\gph)}(v,u') \le 2
\),
so the distance decreases by at least $(L+1)-2 = L-1$.
For $z$ the decrease is $L-2$.
Thus,
\begin{equation}\label{eq:locality_spectral_lb}
  \|\mathcal{D}_\gph - \mathcal{D}_{\mathcal{R}(\gph)} \|_{F} 
  \geq
  \sqrt{(n-1)(L-1)^2 + (L-2)^2}
\end{equation}


\noindent Now consider \gls{laser} with a single snapshot.
If we choose
\begin{equation}\label{eq:rho_choice}
  \rho = \frac{k}{2L}
\end{equation}
then, on average, at most $k$ edges will be added over the chain.
Any chain node distinct from the predecessor of $z$, which we denote by $z'$, has at most two nodes in its $2$-hop neighborhood (it only ``sees'' the path), whereas $z'$ has the whole clique in its $2$-hop neighborhood.
Therefore, the pairwise distance of any two nodes on the chain is changed by at most $k$ and there are $L^2$ such pairs.
For clique-chain pairs, the distance can decrease at most $k+1$ and there are $nL$ such pairs.
Thus,
\begin{equation}\label{eq:locality_laser_ub}
  \|\mathcal{D}_\gph - \mathcal{D}_{\text{\gls{laser}}} \|_{F} 
  \le
  \sqrt{L^2 k^2 + nL (k+1)^2 }
\end{equation}

\noindent
Therefore, by combining \eqref{eq:locality_spectral_lb} and \eqref{eq:locality_laser_ub}, we know that
\eqref{eq:locality_preservation_goal} holds, if
\begin{equation}\label{eq:locality_ineq_raw}
  L^2 k^2 + nL (k+1)^2 
  \le
  (n-1)(L-1)^2 + (L-2)^2
\end{equation}
Rearranging \eqref{eq:locality_ineq_raw} yields
\begin{equation}\label{eq:n_threshold}
  n \ge
  \frac{L^2 k^2 + (L-1)^2 - (L-2)^2}{
    (L-1)^2 - L (k+1)^2
  }
\end{equation}

\noindent
If we choose
\[
  \rho < \frac{\frac{L-1}{\sqrt{L}} - 1}{2L} \approx \frac{1}{2\sqrt{L}}
\]
the denominator in \eqref{eq:n_threshold} is positive, and thus for sufficiently large $n$, \eqref{eq:locality_preservation_goal} holds.
\end{proof}




\section{Methodology}

% \textit{You can change the name of this section as you see fit.}\\
% In this section you should give a description of the methodological aspects of your work, for instance how you modified an existing method to perform a particular task or to overcome a particular limitation. If your project is about reproducibility, here you should describe the method presented in the original paper.

\section{Implementation}

% This section should be structured as follows (from the Reproducibility challenge template):

% ---

% Briefly describe what you did and which resources did you use. E.g. Did you use author's code, did you re-implement parts of the pipeline, how much time did it take to produce the results, what hardware you were using and how long it took to train/evaluate. 

\subsection{Datasets}
% Describe the datasets you used and how you obtained them. 

\subsection{Hyperparameters}
% Describe how you set the hyperparameters and what was the source for their value (e.g. paper, code or your guess). 

\subsection{Experimental setup}
% Explain how you ran your experiments, e.g. the CPU/GPU resources and provide the link to your code and notebooks.

\subsection{Computational requirements}
% Provide information on computational requirements for each of your experiments. For example, the number of CPU/GPU hours and memory requirements. You'll need to think about this ahead of time, and write your code in a way that captures this information so you can later add it to this section. 

\section{Results}

% In this section you should report the results of your work (e.g., the outcome of an empirical analysis). You should be objective and support your statements with empirical evidence.

% \begin{table}[h]
% \small\sf\centering
% \caption{Experimental results (average of 3 runs).}
% \begin{tabular}{l c c}
% \toprule
% Methods & MAE & MSE\\
% \midrule
% \texttt{Baseline1} & $21.23 \pm 1.65$ & $841.36 \pm 12.65$\\
% \texttt{Baseline2} & $15.45 \pm 1.02$ & $652.38 \pm 09.89$\\
% \midrule
% \texttt{Method} & $12.03 \pm 0.35$ & $324.13 \pm 05.56$\\
% \bottomrule
% \end{tabular}
% \label{tab:table}
% \end{table}

% Use figures, plots and tables (like \autoref{tab:table}) to present your results in a nice and readable way.

\section{Discussion and conclusion}

% Here you can express your judgments and draw your conclusions based on the  evidences produced on the previous sections.

% Try to summarize the achievements of your project and its limits, suggesting (when appropriate) possible extensions and future works.

% Bibliography
\renewcommand{\emph}[1]{\textit{#1}} 
\bibliography{bibliography}
\bibliographystyle{unsrtnat}


% \clearpage

\end{document}

\documentclass{gdl}


\usepackage{graphicx}
\usepackage[dvipsnames]{xcolor}
\usepackage{soulutf8}
\usepackage{xparse}

\ExplSyntaxOn
\tl_new:N \__l_SOUL_argument_tl
\cs_set_eq:Nc \SOUL_start:n { SOUL@start }
\cs_generate_variant:Nn \SOUL_start:n { V }
\cs_set_protected:cpn {SOUL@start} #1
 {
  \tl_set:Nn \__l_SOUL_argument_tl { #1 }
  \regex_replace_all:nnN
   { \c{\(} (.*?) \c{\)} } % look for \(...\) (lazily)
   { \cM\$ \1 \cM\$ }      % replace with $...$
   \__l_SOUL_argument_tl
  \SOUL_start:V \__l_SOUL_argument_tl % do the usual
 }
\ExplSyntaxOff

% save soul's \hl under a private name
\let\SOULhl\hl
\renewcommand{\hl}[1]{\SOULhl{#1}} % keep plain \hl working if you want

% punchy highlighter colors
\definecolor{HLgreen}{HTML}{77DD77}
\definecolor{HLyellow}{HTML}{FFFF66}
\definecolor{HLorange}{HTML}{FFB347}
\definecolor{HLred}{HTML}{FF6961}
\definecolor{HLpink}{HTML}{FFB6C1}
\definecolor{HLturquoise}{HTML}{40E0D0}

% master highlight macro: \hl[level]{text}, default = green
\RenewDocumentCommand{\hl}{O{1} m}{%
  \begingroup
  \IfEqCase{#1}{%
    {1}{\sethlcolor{HLgreen}\SOULhl{#2}}%
    {2}{\sethlcolor{HLyellow}\SOULhl{#2}}%
    {3}{\sethlcolor{HLorange}\SOULhl{#2}}%
    {4}{\sethlcolor{HLred}\SOULhl{#2}}%
    {5}{\sethlcolor{red}\SOULhl{#2}}%
    {6}{\sethlcolor{HLturquoise}\SOULhl{#2}}%
  }[\PackageError{hl}{Undefined highlight level: #1}{}]%
  \endgroup
}










\usepackage{amsmath, amssymb, amsthm}
\usepackage{mathtools}

\usepackage{mathdots} % without this package, only \vdots and \ddots are taken from the text font (not the math font), which looks bad if the text font is different from the math font
\usepackage{nicematrix}


\usepackage{booktabs}
\usepackage{array}













\usepackage{pifont}


\usepackage{tikz}
\usetikzlibrary{arrows, arrows.meta, shapes, positioning, calc, fit, patterns, intersections, math, 3d, tikzmark, decorations.pathreplacing, decorations.markings, decorations.text}
\usepackage{tikz-3dplot}
\usepackage{tikzpagenodes}
% \usetikzlibrary{external}
% \tikzexternalize[prefix=tikz/]
\usepackage{pgfplots}


% define colors
\definecolor{funblue}{rgb}{0.10, 0.35, 0.66}
\definecolor{alizarincrimsonred}{rgb}{0.85, 0.17, 0.11}
\definecolor{amethyst}{rgb}{0.6, 0.4, 0.8}
\definecolor{mypurple}{rgb}{128, 0, 128} 
\definecolor{highlightpurple}{rgb}{0.6, 0.0, 0.6}
\renewcommand{\emph}[1]{\textcolor{highlightpurple}{#1}}

\usepackage{thmtools}

\newlength{\thmspace} \setlength{\thmspace}{3pt plus 1pt minus 1pt} 

\declaretheoremstyle[headfont=\bfseries, bodyfont=\normalfont, spaceabove=\thmspace, spacebelow=\thmspace;/, qed=\ensuremath{\vartriangleleft}, postheadspace=1em]{assertionstyle}
\declaretheorem[style=assertionstyle, name=Theorem]{theorem}
\declaretheorem[style=assertionstyle, name=Lemma, sibling=theorem]{lemma}
\declaretheorem[style=assertionstyle, name=Corollary, sibling=theorem]{corollary}
\declaretheorem[style=assertionstyle, name=Proposition, sibling=theorem]{proposition}
\declaretheorem[style=assertionstyle, name=Conjecture, sibling=theorem]{conjecture}
\declaretheorem[style=assertionstyle, name=Claim, sibling=theorem]{claim}
\declaretheorem[style=assertionstyle, name=Fact, sibling=theorem]{fact}

\declaretheoremstyle[headfont=\bfseries, bodyfont=\normalfont, spaceabove=\thmspace, spacebelow=\thmspace, qed=\ding{45}, postheadspace=1em]{definitionstyle}
\declaretheorem[style=definitionstyle, name=Definition]{definition}
\declaretheorem[style=definitionstyle, name=Problem, sibling=definition]{problem}


% \declaretheoremstyle[headfont=\bfseries, bodyfont=\normalfont, spaceabove=6pt, spacebelow=6pt, qed=\ensuremath{\blacktriangleleft}, postheadspace=1em]{definitionstyle}
% \declaretheorem[style=definitionstyle, name=Definition]{definition}

\declaretheoremstyle[headfont=\bfseries, bodyfont=\normalfont, spaceabove=6pt, spacebelow=6pt, qed=\ensuremath{\square}, postheadspace=1em]{proofstyle}
\let\proof\relax \let\endproof\relax
\declaretheorem[style=proofstyle, name=Proof, numbered=no]{proof}

\declaretheoremstyle[headfont=\bfseries\color{funblue}, bodyfont=\normalfont\normalsize, spaceabove=\thmspace, spacebelow=\thmspace, qed=\ensuremath{\color{funblue}\blacktriangleleft}, postheadspace=1em]{examplestyle}
\declaretheorem[style=examplestyle, name=Example,]{example}

\declaretheoremstyle[headfont=\bfseries, bodyfont=\normalfont\normalsize, spaceabove=\thmspace, spacebelow=\thmspace, qed=\ensuremath{\blacktriangleleft}, postheadspace=1em]{remarkstyle}
\declaretheorem[style=remarkstyle, name=Remark,]{remark}

\declaretheoremstyle[headfont=\color{alizarincrimsonred}\bfseries, bodyfont=\normalfont\normalsize, spaceabove=\thmspace, spacebelow=\thmspace, qed=\ensuremath{\color{alizarincrimsonred}\blacktriangleleft}, postheadspace=1em]{cautionstyle}
\declaretheorem[style=cautionstyle, name=Caution, sibling=remark]{caution}

\declaretheoremstyle[headfont=\bfseries, bodyfont=\normalfont\footnotesize, spaceabove=\thmspace, spacebelow=\thmspace, postheadspace=1em]{smallremarkstyle}
\declaretheorem[style=smallremarkstyle, name=Remark, sibling=remark]{smallremark}

\newenvironment{verticalhack}
  {\begin{array}[b]{@{}c@{}}\displaystyle}
  {\\\noalign{\hrule height0pt}\end{array}} % to make qed symbol aligned


\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage[italicComments=false]{algpseudocodex} % macht probleme mit TikZ externalization
\newcommand*{\algorithmautorefname}{Algorithm}


% commands for functions etc.
\newcommand{\im}{\operatorname{Im}}
\newcommand*\matrspace{0.8mu}
\newcommand{\matr}[1]{%
  \mspace{\matrspace}%
  \underline{%
    \mspace{-\matrspace}%
    \smash[b]{\boldsymbol{#1}}%
    \mspace{-\matrspace}%
  }%
  \mspace{\matrspace}%
}
\newcommand{\vect}[1]{\vec{\boldsymbol{#1}}}





% commands for derivatives
\newcommand{\dif}{\mathrm{d}}

% commands for number sets
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}

% commands for probability
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\Cov}{\operatorname{Cov}}
\newcommand{\Exp}{\operatorname{E}}
% \newcommand{\P}{\operatorname{P}} % this is already defined in amsmath/amsopn
\newcommand{\Prob}{\operatorname{P}}
\newcommand{\numof}{\ensuremath{\# \,}} % number of elements in a set
\newcommand{\blackheight}{\operatorname{bh}}

% \algnewcommand{\LeftComment}[1]{\(\triangleright\) #1}
\algnewcommand{\TO}{, \ldots ,}
\algnewcommand{\DOWNTO}{, \ldots ,}
\algnewcommand{\OR}{\vee}
\algnewcommand{\AND}{\wedge}
\algnewcommand{\NOT}{\neg}
\algnewcommand{\LEN}{\operatorname{len}}
\algnewcommand{\tru}{\ensuremath{\mathrm{\texttt{true}}}}
\algnewcommand{\fals}{\ensuremath{\mathrm{\texttt{false}}}}
\algnewcommand{\append}{\circ}

\algnewcommand{\nil}{\ensuremath{\mathrm{\textsc{nil}}}}
\algnewcommand{\red}{\ensuremath{\mathrm{\textsc{red}}}}
\algnewcommand{\black}{\ensuremath{\mathrm{\textsc{black}}}}
\algnewcommand{\gray}{\ensuremath{\mathrm{\textsc{gray}}}}
\algnewcommand{\white}{\ensuremath{\mathrm{\textsc{white}}}}

% \newcommand{\attribute}[1]{\ensuremath{\mathtt{#1}}}
\newcommand{\attrib}[2]{\ensuremath{#1\mathtt{.}\mathtt{#2}}} % object.field with field in math typewriter (like code)
\newcommand{\attribnormal}[2]{\ensuremath{#1\mathtt{.}#2}} 











%% commands from the paper
\newcommand{\gph}{G}
\newcommand{\MPNN}{\text{MPNN}}
\newcommand{\cheeg}{{h}_{\text{Cheeg}
}}
\newcommand{\res}{\text{Res}}
\newcommand{\V}{{V}}
\newcommand{\E}{{E}}
\newcommand{\GAMma}{\boldsymbol{\Gamma}}
%\newcommand{\MPNN}{\mathrm{MPNN}}
\newcommand{\Hi}{\mathbf{H}}
\newcommand{\Hit}{\mathbf{H}^{(t)}}
\newcommand{\up}{\texttt{up}}
\newcommand{\rs}{\text{r}}
\newcommand{\mpas}{\text{a}}
\newcommand{\agg}{\texttt{agg}}
\newcommand{\com}{\text{com}}
\newcommand{\colfirst}{\textcolor{red}}
\newcommand{\colsecond}{\textcolor{blue}}
\newcommand{\colthird}{\textcolor{violet}}
% \newcommand{\MPNN}{\text{MPNN}}
\newcommand{\eigen}{\boldsymbol{\psi}}
\newcommand{\cost}{{c}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\DELta}{\boldsymbol{\Delta}}
\newcommand{\KER}{\mathcal{K}}
\newcommand{\OMEga}{\boldsymbol{\Omega}}
\newcommand{\Anorm}{\boldsymbol{\text{A}}}
\newcommand{\tel}{\text{MPNN}_{\text{tel}}}
\newcommand{\poly}{\text{p}}
\newcommand{\ourname}{\text{telescopic}-\text{MPNN}}
\newcommand{\oper}{\boldsymbol{\text{S}}_{\rs,\mpas}}
\newcommand{\xb}{\vb{x}}
\newcommand{\Xb}{\mathbf{X}}
\newcommand{\Wb}{\vb{W}}
\newcommand{\Hb}{\mathbf{H}}
\newcommand{\Ab}{\mathbf{A}}
\newcommand{\Ib}{\mathbf{I}}
\newcommand{\hb}{\mathbf{h}}
\newcommand{\phib}{\boldsymbol{\phi}}
\newcommand{\pepfunc}{\texttt{Peptides-func}\xspace}
\newcommand{\pepstruct}{\texttt{Peptides-struct}\xspace}











% \usepackage[colorlinks=true, linkcolor=blue, urlcolor=blue, citecolor=green]{hyperref}
% \usepackage{hyperref} % for printing use this version (without colorlinks)
\usepackage[
  pdfauthor={Fabian Bosshard},
  pdftitle={USI - Locality-Aware Graph Rewiring in GNNs - Course Summary},
  pdfkeywords={USI, locality-aware graph rewiring in gnns, course summary, informatics},
  colorlinks=false,        % don't wrap links in a colour
  pdfborder={0 0 0}        % no border around links
]{hyperref}
\usepackage[
  type     = {CC},
  modifier = {by},
  version  = {4.0},
]{doclicense}
\usepackage{cleveref}



% load glossaries *after* hyperref
\usepackage[acronym,              % create an “acronym” glossary
            nomain,               % omit the main glossary (only acronyms)
            toc,                  % add list of acronyms to the ToC
            nonumberlist,         % omit page list in the printed glossary
            automake,
            nopostdot, nogroupskip, style=super, nonumberlist
           ]{glossaries-extra}


% choose how the first appearance looks:
\setabbreviationstyle[acronym]{long-short}

% must be issued once *after* loading glossaries
\makeglossaries


\newacronym{gdl}{GDL}{Graph Deep Learning}
\newacronym{gnn}{GNN}{Graph Neural Network}
\newacronym{gcn}{GCN}{Graph Convolutional Network}
\newacronym{gin}{GIN}{Graph Isomorphism Network}
\newacronym{gat}{GAT}{Graph Attention Network}
\newacronym{gso}{GSO}{Graph Shift Operator}
\newacronym{mpnn}{MPNN}{Message Passing Neural Network}
\newacronym{laser}{LASER}{Locality-Aware Sequential Rewiring}

% experimental details 
\newacronym{oom}{OOM}{Out Of Memory}
\newacronym{ap}{AP}{Average Precision}
\newacronym{mae}{MAE}{Mean Absolute Error}
\newacronym{mrr}{MRR}{Mean Reciprocal Rank}
\newacronym{lrgb}{LRGB}{Long Range Graph Benchmark}
\newacronym{sdrf}{SDRF}{Stochastic Discrete Ricci Flow}
\newacronym{borf}{BORF}{Batch Ollivier-Ricci Flow}
\newacronym{fosr}{FOSR}{First-Order Spectral Rewiring}
\newacronym{gtr}{GTR}{Greedy Total Resistance rewiring}







% CHANGE THESE
\def\groupid{FBFRJT} % Fabian Bosshard, Fanny Rorri, Jimena Tagle
\def\projectid{LASER}

\begin{document}

% CHANGE THIS
\title{Locality-Aware Graph Rewiring in GNNs}

% CHANGE THIS
\author{%
Fabian Bosshard, Fanny Rorri, Laura Jimena Tagle Muñoz \\
\texttt{\{\href{mailto:fabian.bosshard@usi.ch}{fabian.bosshard}, \href{mailto:fanny.rorri@usi.ch}{fanny.rorri}, \href{mailto:jimena.tagle@usi.ch}{jimena.tagle}\}@usi.ch}
}

\begin{abstract}
% Concise and self-contained description of your project, motivation and main findings.

% % Delete the following part before submitting the report
% \begin{center}
%     \sf\large\color{red} GENERAL NOTES
% \end{center}

% The report should be written as an article intended to present the findings of your work. Your aim should be to be clear and objective, substantiating your claims with references or empirical/theoretical evidence.
% We are well aware of the fact that carrying out machine learning experiments might be difficult and that often the final performance might be disappointing. For this reason, you will not be evaluated solely on quantitative aspect of your work, but mainly on the quality of your analysis and report.
% The length of the report should be between 4 and 8 pages (without considering references).
\end{abstract}

\maketitle


\tableofcontents


% table of acronyms (remember everything is in build folder)
\printglossary[type=\acronymtype, title={List of Acronyms}]




\section{Introduction}

% Here you should clarify the context of your project and the problem you are dealing with. 
% You should also make a brief summary of the main results and contributions (i.e., if you tried to replicate the results of an existing paper you should say if you were successful or not). 
% The introduction should help the reader to follow along for the rest of the paper.



\gls{laser}


\section{Background}

Let $G = (V, E)$ be an undirected graph with the
adjacency matrix $\matr{A} \in \R^{n\times n}$
\begin{equation}\label{eq:adjacency_matrix}
    \matr{A}_{uv}
    =
    \begin{cases}
        1 & (u,v) \in E\\
        0 & (u,v) \notin E
    \end{cases}
\end{equation}

The \emph{diagonal degree matrix} $\matr{D}\in\R^{n\times n}$ is defined by
\begin{equation}\label{eq:degree_matrix}
    \matr{D}_{uv}
    =
    \begin{cases}
        d_u &  u = v\\
        0 & u \ne v
    \end{cases}
\end{equation}
i.e. $\matr{D}$ simply places all node degrees on the diagonal.




\subsection{normalized adjacency and multi-hop propagation}


\begin{definition}\label{def:normalized_adjacency}
The \emph{symmetrically normalized adjacency matrix} is
\begin{equation}\label{eq:normalized_adjacency}
    {\color{Red}\boxed{\color{black}
    \hat{\matr{A}}
    =
    \matr{D}^{-1/2}\matr{A}\matr{D}^{-1/2}
    }}
\end{equation}
or, entrywise,
\[
\begin{verticalhack}
    \hat{\matr{A}}_{uv}
    =
    \begin{cases}
        \dfrac{1}{\sqrt{d_u d_v}} & (u,v) \in E\\[0.5ex]
        0 & (u,v) \notin E
    \end{cases}
    \end{verticalhack}
    \qedhere
\]
\end{definition}

\begin{proposition}[multi-hop propagation]\label{prop:multi_hop_propagation}
The entry $(\hat{\matr{A}}^{k})_{vu}$ can be computed explicitly as follows:
\begin{equation}\label{eq:multi_hop_propagation}
    (\hat{\matr{A}}^{k})_{vu}
    =
    \sum_{\pi}
    \prod_{(x, y) \in E_\pi} \frac{1}{\sqrt{d_x d_y}}
\end{equation}
the sum is over all walks $\pi = (v, \ldots, u)$ of length $k$ from $v$ to $u$ and the product is over the edges \(E_\pi = \{(v, u_1), \ldots, (u_{k-1}, u)\}\) on the walk.
\end{proposition}



\begin{corollary}\label{cor:multi_hop_propagation_single_path}
Let $v,u \in V$ with $r = d_{G}(v,u)$, where $d_{G}(\cdot,\cdot)$ denotes the shortest-path distance.
Assume there is exactly one path
\[
    (v, u_{1}, \ldots, u_{r-1}, u)
\]
of length $r$ between $v$ and $u$:
\[
\begin{tikzpicture}[
  font=\footnotesize,
  scale=0.4,
  vertex/.style={
    circle,
    draw,
    fill=green!40,
    minimum size=10pt,
    inner sep=0,
    label={
        [anchor=base, label distance=5pt]
        below:{#1} % parentheses necessary, otherwise the '=' breaks TikZ parsing
    }
  },
  measure/.style={|-|}
]

\node[vertex={$v$}] (v) at (0, 0) {};
\node[vertex={$u_{1}$}] (u1) at (3, 0) {};
\node[vertex={$u_{r-1}$}] (urm1) at (9, 0) {};
\node[vertex={$u$}] (u) at (12, 0) {};


\draw[] (v) -- (u1);
\draw[] (u1) -- ($(u1)!1/3!(urm1)$);
\draw[dotted] ($(u1)!1/3!(urm1)$) -- ($(u1)!2/3!(urm1)$);
\draw[] ($(u1)!2/3!(urm1)$) -- (urm1);
\draw[] (urm1) -- (u);


\def\offset{-0.5cm};
\draw[measure] ($(v) + (0, -1) + (0, \offset)$) -- node[below, outer sep = 2pt, inner sep = 0] {$d_{G}(v,u) = r$} ($(u) + (0, -1) + (0, \offset)$);


\end{tikzpicture}
\]
Then
\begin{equation}\label{eq:multi_hop_propagation_single_path}
    \begin{aligned}
    (\hat{\matr{A}}^{r})_{vu}
    &=
    \frac{1}{\sqrt{d_v d_{u_{1}}}} \cdot
    \prod_{i=1}^{r-2} \frac{1}{\sqrt{d_{u_{i}} d_{u_{i+1}}}} \cdot
    \frac{1}{\sqrt{d_{u_{r-1}} d_{u}}} \\
    &=
    \frac{1}{\sqrt{d_v d_u}} \prod_{i=1}^{r-1} \frac{1}{d_{u_i}}
    \end{aligned}
\end{equation}
\end{corollary}



\subsubsection{distance layers and layer degrees}

% For \gls{laser} and related methods it is useful to separate connections by graph distance.

\begin{definition}\label{def:distance_layer_adjacency}
For $\ell \in \N_0$, we define the \emph{distance-($\ell+1$) adjacency matrix} $\matr{A}_{\ell} \in \R^{n\times n}$ by
\begin{equation}\label{eq:distance_layer_adjacency}
    \bigl(\matr{A}_{\ell}\bigr)_{uv}
    =
    \begin{cases}
        1 & d_{G}(u,v) = \ell+1\\
        0 & \text{otherwise}
    \end{cases}
\end{equation}
where $d_{G}(u,v)$ is the shortest-path distance.
The corresponding \emph{layer degree} of a node $v$ at distance level $\ell$ is
\begin{equation}\label{eq:layer_degree}
    d_{v,\ell}
    =
    \sum_{u \in V} \bigl(\matr{A}_{\ell}\bigr)_{vu},
\end{equation}
i.e.\ the number of nodes at graph distance $\ell+1$ from $v$.
Let $\matr{D}_{\ell}$ be the diagonal matrix with $(\matr{D}_{\ell})_{vv} = d_{v,\ell}$.
The \emph{normalized distance-$(\ell+1)$ adjacency} is
\begin{equation}\label{eq:normalized_distance_layer_adjacency}
    {\color{Red}\boxed{\color{black}
    \hat{\matr{A}}_{\ell}
    =
    \matr{D}_{\ell}^{-1/2} \matr{A}_{\ell} \matr{D}_{\ell}^{-1/2}
    }}
\end{equation}
so that
\[
\begin{verticalhack}
\bigl(\hat{\matr{A}}_{\ell}\bigr)_{uv}
=
\begin{cases}
    \frac{1}{\sqrt{d_{u,\ell} d_{v,\ell}}} & d_{G}(u,v) = \ell+1 \\
    0 & \text{otherwise}
\end{cases}
\end{verticalhack}
\qedhere
\]
\end{definition}

Finally, we denote by
\begin{equation}\label{eq:minimum_degree}
    d_{\min}
    =
    \min_{v \in V} d_v
\end{equation}
the \emph{minimum node degree} in the graph.
% Bounds involving powers of $d_{\min}$ quantify how the worst-case degree along a path affects information flow and Jacobian norms in \gls{gcn}- and \gls{laser}-type architectures.






\subsection{graph Laplacian}

\begin{definition}\label{def:graph_laplacian}
The \emph{combinatorial graph Laplacian} is
\begin{equation}\label{eq:graph_laplacian}
    \matr{L} = \matr{D} - \matr{A}
\end{equation}
and the \emph{normalized graph Laplacian} is
\begin{equation}\label{eq:normalized_graph_laplacian}
    {\color{Red}\boxed{\color{black}
    \hat{\matr{L}}
    =
    \matr{D}^{-1/2}\matr{L}\matr{D}^{-1/2}
    \overset{\text{\eqref{eq:graph_laplacian}}}{=}
    \matr{D}^{-1/2}(\matr{D} - \matr{A})\matr{D}^{-1/2}
    \overset{\text{\eqref{eq:normalized_adjacency}}}{=}
    \matr{I}_{n} - \hat{\matr{A}}
    }}
\end{equation}
It is symmetric and positive semidefinite, and its eigenvalues satisfy
\[
    0 = \lambda_{0} \le \lambda_{1} \le \dots \le \lambda_{n-1}
\]
$\lambda_1$ is called the \emph{spectral gap}.
The number of zero eigenvalues (i.e., the multiplicity of the \(0\) eigenvalue) equals the number of connected components of the graph.
\end{definition}

To understand \autoref{def:graph_laplacian}, consider a function $f\colon V \to \R$.
Denote by $\vect{f} \in \R^{n}$ the vector whose $v$-th entry is $f(v)$.
Then
% \begin{equation}\label{eq:laplacian_action_on_function}
% (\matr{L}\vect{f})_v
% =
% d_v f(v) - \sum_{(u,v) \in E} f(u)
% \end{equation}
% i.e., $(\matr{L}\vect{f})_v$ is (up to the factor $d_v$) the difference between the value at $v$ and the sum of the values at its neighbors.
% For the normalized Laplacian, we have
\begin{equation}\label{eq:normalized_laplacian_action_on_function}
(\hat{\matr{L}}\vect{f})_v
=
f(v)
-
\frac{1}{\sqrt{d_v}}
\sum_{(u,v) \in E}
\frac{f(u)}{\sqrt{d_u}}
\end{equation}
i.e., $(\hat{\matr{L}}\vect{f})_v$ is the value at $v$ minus a degree-normalized average of the neighbors.
This is why the Laplacian is often viewed as a \emph{discrete second derivative} on the graph:
\hl[2]{it measures how much $f$ at $v$ deviates from its neighborhood}.
Another important identity is the quadratic form
\begin{equation}\label{eq:laplacian_quadratic_form}
    \vect{f}^{\top}\matr{L}\vect{f}
    =
    \frac{1}{2}
    \sum_{(u,v) \in E}
    \bigl(f(u) - f(v)\bigr)^{2}
\end{equation}
which shows that \(\matr{L}\) (and hence also \(\hat{\matr{L}}\)) is positive semidefinite, since the right-hand side is always nonnegative.
Moreover, \eqref{eq:laplacian_quadratic_form} is small exactly when $f$ varies slowly across edges, so the Laplacian encodes the \emph{smoothness} of functions on the graph.




\subsection{Cheeger inequality}

The \emph{Cheeger inequality} relates the spectral gap $\lambda_1$ to the \emph{Cheeger constant} $h(G)$, which measures how difficult it is to separate the graph into two large pieces.  It states, in particular, that
\[
    \tfrac{1}{2}h(G)^2
    \le
    \lambda_1
    \le
    2 h(G),
\]
so a larger spectral gap implies that the graph is more ``well-connected''.


\subsection{effective resistance}
\begin{definition}[effective resistance]\label{def:effective_resistance}
View each edge $(u,v)\in E$ as an electrical resistor of resistance $1\,$\(\Omega\).
The resulting network has a well-defined resistance between any two nodes.

For two nodes $s,t \in V$, the \emph{effective resistance} $R(s,t)$ is defined as the voltage difference needed to send one unit of electrical current from $s$ to $t$.
It can be computed as
\begin{equation}\label{eq:effective_resistance}
{\color{Red}\boxed{\color{black}
    R(s,t)
    =
    (\vect{e}_s - \vect{e}_t)^{\top}
    \matr{L}^{\dagger}
    (\vect{e}_s - \vect{e}_t)
}}
\end{equation}
where $\matr{L}^{\dagger}$ is the Moore--Penrose pseudoinverse of $\matr{L}$ and $\vect{e}_v$ is the standard basis vector of vertex $v$.
\end{definition}


\subsubsection{Interpretation}
If the graph offers many short, parallel paths between $s$ and $t$, then current can flow easily, so $R(s,t)$ is small.
If there are few or long paths, the current is ``bottlenecked'' and $R(s,t)$ is large.
Thus, effective resistance measures how ``well-connected'' two nodes are inside the global geometry of the graph.

\subsubsection{Connection to random walks}
A \emph{random walk} on $G$ is the Markov chain that, from a node $v$, moves to a uniformly random neighbor of $v$.  Its transition matrix is
\begin{equation}\label{eq:random_walk_transition_matrix}
\matr{P} = \matr{D}^{-1}\matr{A}
\end{equation}
so $\matr{P}_{vu} = 1/d_v$ if $(v,u)\in E$.

\medskip
For two nodes $u,v$, the \emph{commute time} $\texttt{CT}(u,v)$ is the expected number of steps for the random walk to start at $u$, reach $v$, and return to $u$ again.  
It can be related to the \nameref{def:effective_resistance} via
\begin{equation}\label{eq:commute_time_effective_resistance}
    \texttt{CT}(u,v)
    =
    2|E|
    R(u,v)
\end{equation}
giving a geometric interpretation of how ``far apart'' two nodes are in terms of random-walk behavior,
i.e. two nodes have small commute time exactly when they have small effective resistance.









\section{Theoretical analysis}\label{sec:theoretical_analysis}


\subsection{Dynamic rewiring paradigm}

The main contribution of \cite{barbero2024laser} is the following idea:
Instead of applying a single rewired graph \(\mathcal{R}(G)\), consider a sequence of rewired graphs
\begin{equation}\label{eq:sequence_rewiring}
\gph = \gph_0 \xhookrightarrow[]{\mathcal{R}} \cdots \xhookrightarrow[]{\mathcal{R}} \gph_{\ell-1} \xhookrightarrow[]{\mathcal{R}} \gph_{\ell} \xhookrightarrow[]{\mathcal{R}} \cdots \xhookrightarrow[]{\mathcal{R}} \gph_L
\end{equation}
where \(G_\ell\) is called the \(\ell\)-snapshot, since we think of \(G_\ell\) as the input graph evolved along a dynamical process for \(\ell\) iterations.


The variant to derive the theoretical results in \cite{barbero2024laser} is a \gls{gcn}-\gls{laser} architecture
\begin{equation}\label{eq:laser-GCN}
\begin{aligned}
% \vect{x}_{v}^{(t)} 
%  =&  \operatorname{ReLU}\Bigg( 
%   \sum_{u\in \mathcal{N}_1(v)\cup\{v\}}\frac{\matr{W}_0^{(t)}\vect{x}_{u}^{(t-1)} }{\sqrt{d_v d_u}}\\
%  &+ 
%  \sum_{\ell = 1}^{L} \sum_{u\in\mathcal{N}^\rho_{\ell+1}(v)}
% \frac{\matr{W}_{\ell}^{(t)} \vect{x}_u^{(t-1)}}{\sqrt{d_{v,\ell} d_{u,\ell}}}
% \Bigg)
\vect{x}_{v}^{(t)} 
 =&  \operatorname{ReLU}\Bigg( 
  \sum_{u\in \mathcal{N}_1(v)\cup\{v\}}
  \frac{1}{\sqrt{d_v d_u}} \cdot \matr{W}_0^{(t)}\vect{x}_{u}^{(t-1)}\\
 &+ 
 \sum_{\ell = 1}^{L} 
 \sum_{u\in\mathcal{N}^\rho_{\ell+1}(v)}
\frac{1}{\sqrt{d_{v,\ell} d_{u,\ell}}} \cdot \matr{W}_{\ell}^{(t)} \vect{x}_u^{(t-1)}
\Bigg)
\end{aligned}
\end{equation}
where \(\matr{W}_{\ell}^{(t)}\), for \(\ell = 0, \ldots, L\), 
are learnable weight matrices and 
\begin{equation}\label{eq:laser-neighborhood}
\mathcal{N}^\rho_{\ell + 1}(v) \subset \mathcal{N}_{\ell + 1}(v) := \{u \in V \mid d_{G_0}(u,v) = \ell + 1\}
\end{equation}
is a \(\rho\)-fraction of nodes at distance exactly \(\ell + 1\) from \(v\) with the lowest connectivity score \(\mu\) in the original graph \(G = G_0\). 
So, for the \(\ell^{\text{th}}\) snapshot \(G_\ell\), \gls{laser} considers \(\rho \cdot d_{v,\ell + 1}\) artificial edges from \(v\) to nodes at distance \(d_{G_0}(u,v) = \ell + 1\).




\subsection{Jacobian sensitivity}

Let \(\matr{J}^{(t)}(v, u) := \partial \vect{x}_v^{(t)} / \partial \vect{x}_u^{(0)}\) be the Jacobian of features after \(t\) layers of message passing on \(G\) using a \gls{gcn} as described in \cite{kipf2016semi},
and similarly let \(\smash{\tilde{\matr{J}}}^{(t)}(v, u)\) be the Jacobian after \(t\) layers using \gls{laser}, as defined in \autoref{eq:laser-GCN}.

\begin{proposition}\label{prop:jacobian_sensitivity}
Let $v,u \in V$ with $r = d_{G_0}(v,u)$, where $G_0$ is the original graph before applying \gls{laser}.
Assume there is exactly one path \(\pi \subset G_0\) of length \(r\) connecting \(v\) and \(u\) (see \autoref{fig:jacobian_sensitivity_path}).


\begin{figure}[h]
\centering
\begin{tikzpicture}[
  font=\footnotesize,
  scale=0.45,
  vertex/.style={
    circle,
    draw,
    fill=green!40,
    minimum size=10pt,
    inner sep=0,
    label={
        [anchor=base, label distance=5pt]
        below:{#1} % parentheses necessary, otherwise the '=' breaks TikZ parsing
    }
  },
  measure/.style={
    |-|,
    >=stealth
  }
]

\node[vertex={$v = u_0$}] (v) at (0, 0) {};
\node[vertex={$u_{\ell-1}$}] (ulm1) at (6, 0) {};
\node[vertex={$j = u_\ell$}] (j) at (9, 0) {};
\node[vertex={$u_{\ell+1}$}] (ulp1) at (12, 0) {};
\node[vertex={$u = u_r$}] (u) at (18, 0) {};

\draw[] (v) -- ($(v)!1/3!(ulm1)$);
\draw[dotted] ($(v)!1/3!(ulm1)$) -- ($(v)!2/3!(ulm1)$);
\draw[] ($(v)!2/3!(ulm1)$) -- (ulm1);

\draw[] (ulm1) -- (j);
\draw[] (j) -- (ulp1);

\draw[] (ulp1) -- ($(ulp1)!1/3!(u)$);
\draw[dotted] ($(ulp1)!1/3!(u)$) -- ($(ulp1)!2/3!(u)$);
\draw[] ($(ulp1)!2/3!(u)$) -- (u);


\draw[blue, densely dashed] (v) to[out=45, in=135] node[pos=0.24, above, rotate=25] {LASER} (j);
\node[above right=0pt of j, rotate=45, outer sep=2pt, inner sep=0pt, anchor=west] {$\in \mathcal{N}_{\ell}^{\rho}(v)$};

\def\offset{-0.5cm};
\draw[measure] ($(v) + (0, -1) + (0, \offset)$) -- node[below, outer sep = 2pt, inner sep = 0] {$d_{G_0}(v,u) = r$} ($(u) + (0, -1) + (0, \offset)$);
\draw[measure] ($(v) + (0, -2) + (0, \offset)$) -- node[below, outer sep = 2pt, inner sep = 0] {$d_{G_0}(v,j) = \ell$} ($(j) + (0, -2) + (0, \offset)$);
\draw[measure, densely dashed] ($(v) + (0, -3) + (0, \offset)$) -- node[below, outer sep = 2pt, inner sep = 0] {$d_{G_{\color{red} \ell - 1}}(v,u) = \color{blue} 1 + \color{black} r - \ell$} ($(u) + (0, -3) + (0, \offset)$);

\end{tikzpicture}
\caption{
Unique shortest path \(\pi \subset G_0\) between \(v\) and \(u\) of length \(r\) with an intermediate node \(j\) at distance \(\ell\) from \(v\).
The dashed blue line indicates the edge added by \gls{laser}.
}
\label{fig:jacobian_sensitivity_path}
\end{figure}


\noindent
Assume \gls{laser} adds an edge between \(v\) and a node \(j \in V_\pi\) with \(d_{G_0}(v,j) = \ell < r\).
Furthermore, assume that all products of weight matrices have unit norm.
Then for all \(m \leq r\) we have
\begin{equation}\label{eq:jacobian_sensitivity}
\left\|\Exp\left[\smash{\tilde{\matr{J}}}^{(r - \ell + 1)}(v, u)\right]\right\|
\geq
\frac{(d_{\min})^{\ell} \sqrt{\frac{d_j}{d_u}}}{\sqrt{d_{v,\ell - 1} d_{j,\ell - 1}}}
\left\|\Exp\left[\matr{J}^{(m)}(v, u)\right]\right\|
\end{equation}
where \(d_{\min} = \min_{v \in V} d_v\) is the minimum degree in \(G_0\).
\end{proposition}


\begin{proof}
In the following, the expectations are with respect to \(\operatorname{ReLU}'\) which is assumed to have probability of success \(p\) for all paths in the computational graph as in \cite{xu2018representation, digiovanni2023over}.
The Jacobian after \(m\) layers of \gls{gcn} is
\[
\left\| \Exp\left[\matr{J}^{(m)}(v, u)\right] \right\|
=
 p \left\| \prod_{k=1}^{m} \matr{W}^{(k)} \right\| (\hat{\matr{A}}^{m})_{vu}
 =
  p (\hat{\matr{A}}^{m})_{vu}
\]
where we used the unit-norm assumption on the weight matrices and \(\hat{\matr{A}}\) denotes the normalized adjacency matrix from \autoref{def:normalized_adjacency}.

For \(m < r\), the expression above vanishes, satisfying \eqref{eq:jacobian_sensitivity}, so we consider \(m = r\) from now on.

With the same assumptions, the Jacobian after \(r - \ell + 1\) layers of \gls{laser} is
\[
\begin{aligned}
\left\| \Exp\left[\smash{\tilde{\matr{J}}}^{(r - \ell + 1)}(v, u)\right] \right\|
=&
  p \left\| \prod_{k=2}^{r - \ell + 1} \matr{W}_0^{(k)}  {\color{blue}\matr{W}_{\ell - 1}^{(1)}} \right\|
  (\hat{\matr{A}}_{\ell - 1})_{vj} (\hat{\matr{A}}^{r - \ell})_{ju} \\
=&
  p (\hat{\matr{A}}_{\ell - 1})_{vj} (\hat{\matr{A}}^{r - \ell})_{ju}
\end{aligned}
\]

As we can see in \autoref{fig:jacobian_sensitivity_path}, we have a unique path
\[
(v = u_0, u_1, \dots, u_{r-1}, u_r = u)
\]
and we assume \(j = u_\ell\).
From \autoref{cor:multi_hop_propagation_single_path}, for the full path from \(v\) to \(u\):
\[
(\hat{\matr{A}}^{r})_{vu}
=
\frac{1}{\sqrt{d_v d_u}}
\prod_{s=1}^{r-1} \frac{1}{d_{u_s}}
\]

For the sub-path from \(j = u_\ell\) to \(u = u_r\) of length \(r-\ell\), the same reasoning gives
\[
(\hat{\matr{A}}^{r-\ell})_{ju}
=
\frac{1}{\sqrt{d_j d_u}}
\prod_{s=\ell+1}^{r-1} \frac{1}{d_{u_s}}.
\]

Now we calculate the ratio
\[
\begin{aligned}
\frac{
\left\| \Exp\left[\smash{\tilde{\matr{J}}}^{(r - \ell + 1)}(v, u)\right] \right\|
}{
\left\| \Exp\left[\matr{J}^{(r)}(v, u)\right] \right\|
}
&=
\frac{(\hat{\matr{A}}_{\ell-1})_{vj}(\hat{\matr{A}}^{r-\ell})_{ju}}{\frac{1}{\sqrt{d_v d_u}}\prod_{s = 1}^{r-1}\frac{1}{d_{u_s}}} \\
&=
\frac{(\hat{\matr{A}}_{\ell-1})_{vj}}{\frac{1}{\sqrt{d_v d_u}}\prod_{s = 1}^{\color{red}\ell\color{black}-1}\frac{1}{d_{u_s}}}
\cdot
\frac{(\hat{\matr{A}}^{r-\ell})_{ju}}{\prod_{s = {\color{red}\ell}}^{r-1}\frac{1}{d_{u_s}}} \\
&\overset{\text{\eqref{eq:multi_hop_propagation_single_path}}}{=}
\color{gray}
\frac{(\hat{\matr{A}}_{\ell-1})_{vj}}{\frac{1}{\sqrt{d_v d_u}}\prod_{s = 1}^{\ell-1}\frac{1}{d_{u_s}}}
\color{black}
\cdot
\frac{\frac{1}{\sqrt{d_j d_u}} \prod_{s=\ell+1}^{r-1} \frac{1}{d_{u_s}}}
{\prod_{s = \ell}^{r-1}\frac{1}{d_{u_s}}} \\
&=
\color{gray}
\frac{(\hat{\matr{A}}_{\ell-1})_{vj}}{\frac{1}{\sqrt{d_v d_u}}\prod_{s = 1}^{\ell-1}\frac{1}{d_{u_s}}}
\color{black}
\cdot
\frac{\frac{1}{\sqrt{d_j d_u}}\prod_{s=\ell+1}^{r-1} \frac{1}{d_{u_s}}}{\frac{1}{d_{u_\ell}}\prod_{s=\ell+1}^{r-1} \frac{1}{d_{u_s}}} \\
&=
\color{gray}
\frac{(\hat{\matr{A}}_{\ell-1})_{vj}}{\frac{1}{\sqrt{d_v d_u}}\prod_{s = 1}^{\ell-1}\frac{1}{d_{u_s}}}
\color{black}
\cdot
\frac{d_{u_\ell}}{\sqrt{d_j d_u}} \\
&=
% \color{gray}
\frac{(\hat{\matr{A}}_{\ell-1})_{vj}}{\frac{1}{\sqrt{d_v d_u}}\prod_{s = 1}^{\ell-1}\frac{1}{d_{u_s}}}
% \color{black}
\cdot
\sqrt{\frac{d_j}{d_u}}
\end{aligned}
\]
% So
% \begin{equation}\label{eq:laser_ratio_exact}
% \frac{(\hat{\matr{A}}_{\ell-1})_{vj}(\hat{\matr{A}}^{r-\ell})_{ju}}
% {\frac{1}{\sqrt{d_v d_u}}\prod_{s = 1}^{r-1}\frac{1}{d_{u_s}}}
% =
% \frac{(\hat{\matr{A}}_{\ell-1})_{vj}}{\frac{1}{\sqrt{d_v d_u}}\prod_{s = 1}^{\ell-1}\frac{1}{d_{u_s}}}
% \cdot
% \sqrt{\frac{d_j}{d_u}}
% \end{equation}
Using
\[
(\hat{\matr{A}}_{\ell-1})_{vj} 
\overset{\text{\eqref{eq:normalized_distance_layer_adjacency}}}{=} 
\frac{1}{\sqrt{d_{v,\ell-1} d_{j,\ell-1}}}
\]
and
\[
\frac{1}{\sqrt{d_v d_u}}\prod_{s=1}^{\ell-1} \frac{1}{d_{u_s}}
\le
\frac{1}{(d_{\min})^\ell}
\]
we obtain
\[
\frac{(\hat{\matr{A}}_{\ell-1})_{vj}}{\frac{1}{\sqrt{d_v d_u}}\prod_{s = 1}^{\ell-1}\frac{1}{d_{u_s}}}
\ge
\frac{(d_{\min})^\ell}{\sqrt{d_{v,\ell-1} d_{j,\ell-1}}}
\]
which, combined with the previous equation, gives
\[
\frac{
\left\| \Exp\left[\smash{\tilde{\matr{J}}}^{(r - \ell + 1)}(v, u)\right] \right\|
}{
\left\| \Exp\left[\matr{J}^{(r)}(v, u)\right] \right\|
}
\ge
\frac{(d_{\min})^\ell}{\sqrt{d_{v,\ell-1} d_{j,\ell-1}}}
\cdot
\sqrt{\frac{d_j}{d_u}}
\]
concluding the proof.
\end{proof}




\subsection{Preservation of locality}


\begin{proposition}\label{prop:locality_preservation}

Following \cite{digiovanni2023over}, we compare how much a rewiring perturbs the shortest-path distances by means of the Frobenius norm
\[
    \bigl\|\mathcal{D}_G - \mathcal{D}_{G'}\bigr\|_{F},
    \qquad
    (\mathcal{D}_G)_{uv} = d_{G}(u,v).
\]




\begin{figure}[h]
\centering
\begin{tikzpicture}[
  font=\footnotesize,
  scale=0.5,
  vertex/.style={
    circle,
    draw,
    fill=green!40,
    minimum size=10pt,
    inner sep=0
  },
  measure/.style={
    |-|,
    >=stealth
  }
]

\def\zLocation{10cm}
\def\cliqueRadius{2.5cm}
\coordinate (cliqueCenter) at ($(\zLocation,0) + (\cliqueRadius,0)$);

\node[circle, draw, dashed, fill=orange!20, minimum size=\cliqueRadius, inner sep=0pt] (clique) at (cliqueCenter) {};
\node at (cliqueCenter) {$|\texttt{clique}| = n$};


\node[vertex, label={[anchor=base, label distance=7pt]below:{$v$}}] (v) at (0, 0) {};
\node[vertex, label={[anchor=base, label distance=7pt]below:{$z'$}}] (zprime) at ($(\zLocation,0) - (0.2*\zLocation,0)$) {};
\node[vertex, label={[anchor=base, label distance=7pt, xshift=-6pt, yshift=3]below:{$z$}}] (z) at (\zLocation, 0) {};

\draw[] (v) -- ($(v)!1/5!(zprime)$);
\draw[dotted] ($(v)!1/5!(zprime)$) -- ($(v)!4/5!(zprime)$);
\draw[] ($(v)!4/5!(zprime)$) -- (zprime);

\draw[] (zprime) -- (z);

\def\offset{-0.5cm};
\draw[measure] ($(v) + (0, -1) + (0, \offset)$) -- node[below, outer sep = 2pt, inner sep = 0] {$L$} ($(z) + (0, -1) + (0, \offset)$);
\end{tikzpicture}
\caption{
Lollipop graph with a chain of length \(L\) attached to a clique of size \(n\).
}
\label{fig:lollipop_graph}
\end{figure}



\end{proposition}

\paragraph{Setup.}
Let $G$ be a lollipop graph consisting of a path of length $L$ attached to a clique of size $n$.
Let $v$ be the endpoint of the path, $z$ the junction between path and clique, and let the other $n-1$ clique nodes be “interior” nodes.

\subsubsection*{Spectral rewiring: lower bound}

Consider a spectral rewiring $\mathcal{R}$ that adds one edge between the pair with largest effective resistance, which in this graph is $(v,u)$ for some interior clique node $u$.
Before rewiring,
\[
    d_{G}(v,u) = L+1,
    \qquad
    d_{G}(v,z) = L.
\]
After adding $(v,u)$ we have
\[
    d_{\mathcal{R}(G)}(v,u) = 1,
    \qquad
    d_{\mathcal{R}(G)}(v,w) \le 2 \ \text{for all interior clique } w,
    \qquad
    d_{\mathcal{R}(G)}(v,z) \le 2.
\]
Hence, for each of the $n-1$ interior clique nodes
\[
    \bigl|d_{G}(v,w) - d_{\mathcal{R}(G)}(v,w)\bigr|
    \;\ge\; (L+1) - 2 = L-1,
\]
and for $z$
\[
    \bigl|d_{G}(v,z) - d_{\mathcal{R}(G)}(v,z)\bigr|
    \;\ge\; L - 2.
\]
Restricting the Frobenius sum to these $n$ pairs yields
\begin{equation}\label{eq:frob_spectral_lower}
    \bigl\|\mathcal{D}_G - \mathcal{D}_{\mathcal{R}(G)} \bigr\|_{F}^{2}
    \;\ge\;
    (n-1)(L-1)^{2} + (L-2)^{2},
\end{equation}
and therefore
\begin{equation}\label{eq:frob_spectral_lower_root}
    \bigl\|\mathcal{D}_G - \mathcal{D}_{\mathcal{R}(G)} \bigr\|_{F}
    \;\ge\;
    \sqrt{(n-1)(L-1)^{2} + (L-2)^{2}}.
\end{equation}

\subsubsection*{LASER: upper bound}

Now consider \gls{laser} with a single snapshot restricted to distance-$2$ edges.
On the path, every node except the penultimate one $z'$ has at most two distance-$2$ neighbours; $z'$ has the whole clique at distance $2$.
If we choose $\rho$ such that
\begin{equation}\label{eq:rho_k_relation}
    \rho = \frac{k}{2L},
\end{equation}
then the \emph{expected} number of new edges incident to path nodes is $k$.
Assuming that at most $k$ such edges are added, we obtain uniform bounds on the distance changes.

\paragraph{Path--path pairs.}
Each new edge connects two nodes at distance $2$, so using it on a shortest path can shorten that path by at most $1$.
Any path between two nodes $x,y$ on the path can traverse at most $k$ new edges, hence
\[
    \bigl|d_{G}(x,y) - d_{\text{LASER}}(x,y)\bigr| \le k
    \quad\text{for all path nodes } x,y.
\]
There are at most $L^{2}$ ordered path--path pairs, so their contribution to the Frobenius norm satisfies
\begin{equation}\label{eq:frob_laser_chain}
    \sum_{\substack{x,y \text{ on path}}}
    \bigl(d_{G}(x,y) - d_{\text{LASER}}(x,y)\bigr)^{2}
    \;\le\;
    L^{2} k^{2}.
\end{equation}

\paragraph{Path--clique pairs.}
In the worst case, one of the $k$ new edges connects $z'$ directly to some clique node.
Then any shortest path between a path node $x$ and a clique node $c$ can use at most $k$ shortcuts along the path and one shortcut to enter the clique.
Thus
\[
    \bigl|d_{G}(x,c) - d_{\text{LASER}}(x,c)\bigr|
    \;\le\; k+1
    \quad\text{for all } x \text{ on the path and } c \text{ in the clique}.
\]
There are $nL$ such ordered pairs, giving
\begin{equation}\label{eq:frob_laser_chain_clique}
    \sum_{\substack{x \text{ on path}\\ c \text{ in clique}}}
    \bigl(d_{G}(x,c) - d_{\text{LASER}}(x,c)\bigr)^{2}
    \;\le\;
    nL (k+1)^{2}.
\end{equation}

Distances between clique nodes are not increased by adding local edges from the path and are at most mildly decreased; their contribution can be bounded and is dominated by the two terms above.
Hence we obtain the global upper bound
\begin{equation}\label{eq:frob_laser_upper}
    \bigl\|\mathcal{D}_G - \mathcal{D}_{\text{LASER}} \bigr\|_{F}^{2}
    \;\le\;
    L^{2} k^{2} + nL (k+1)^{2},
\end{equation}
and therefore
\begin{equation}\label{eq:frob_laser_upper_root}
    \bigl\|\mathcal{D}_G - \mathcal{D}_{\text{LASER}} \bigr\|_{F}
    \;\le\;
    \sqrt{L^{2} k^{2} + nL (k+1)^{2}}.
\end{equation}

\subsubsection*{Comparison and corrected parameter constraints}

A sufficient condition for \gls{laser} to be more locality-preserving than the spectral rewiring is
\[
    \bigl\|\mathcal{D}_G - \mathcal{D}_{\text{LASER}} \bigr\|_{F}
    \;\le\;
    \bigl\|\mathcal{D}_G - \mathcal{D}_{\mathcal{R}(G)} \bigr\|_{F}.
\]
Using \eqref{eq:frob_spectral_lower_root} and \eqref{eq:frob_laser_upper_root}, this is guaranteed if
\begin{equation}\label{eq:ineq_nkL_raw}
    L^{2} k^{2} + nL (k+1)^{2}
    \;\le\;
    (n-1)(L-1)^{2} + (L-2)^{2}.
\end{equation}
Rearranging~\eqref{eq:ineq_nkL_raw} and solving for $n$ gives
\begin{equation}\label{eq:n_bound_correct}
    n
    \;\ge\;
    \frac{k^{2} L^{2} + 2L - 3}
         {L^{2} - Lk^{2} - 2Lk - 3L + 1},
\end{equation}
provided the denominator is positive.
A sufficient condition for the denominator to be positive is
\begin{equation}\label{eq:k_condition}
    k^{2} < \frac{L}{4}, \qquad L \ge 8,
\end{equation}
which we verified analytically for $k \in \N$.
Using the relation $\rho = k/(2L)$ from~\eqref{eq:rho_k_relation}, the constraint \eqref{eq:k_condition} is equivalent to
\begin{equation}\label{eq:rho_bound_correct}
    \rho^{2} < \frac{1}{16 L}
    \quad\Longleftrightarrow\quad
    \rho < \frac{1}{4\sqrt{L}}.
\end{equation}

In summary, on lollipop graphs with $L \ge 8$, sufficiently small sampling rate
$\rho < 1/(4\sqrt{L})$ and $n$ satisfying~\eqref{eq:n_bound_correct}, we obtain
\begin{equation}\label{eq:final_locality_comparison}
    \bigl\|\mathcal{D}_G - \mathcal{D}_{\text{LASER}} \bigr\|_{F}
    \;\le\;
    \bigl\|\mathcal{D}_G - \mathcal{D}_{\mathcal{R}(G)} \bigr\|_{F},
\end{equation}
i.e.\ \gls{laser} perturbs the shortest-path distances less than the spectral effective-resistance-based rewiring.

Compared to the original derivation, we note that:
(i) the path--clique bound should be interpreted as a worst-case inequality $\le k+1$ rather than an equality;
(ii) the algebraic manipulation leading to the bound on $n$ in~\eqref{eq:n_bound_correct} fixes a sign error in the denominator; and
(iii) the correct constraint on $\rho$ is~\eqref{eq:rho_bound_correct}, which differs by a factor of $2$ from the value stated in the paper.



\section{Related works}

% Give a brief summary of (some) existing methods that are related to you project. For instance, you can refer to~\citet{gilmer2017neural}, or simply~\cite{gilmer2017neural}, for introducing Message Passing Neural Networks. In this section it is important to provide readers references to the current state of the art and the foundations of the presented method. 

% N.B.: When referencing a different approach, it is not necessary to provide a detailed description, only one/two brief sentences are enough. The interested readers can eventually read the referenced work. 

\section{Methodology}

% \textit{You can change the name of this section as you see fit.}\\
% In this section you should give a description of the methodological aspects of your work, for instance how you modified an existing method to perform a particular task or to overcome a particular limitation. If your project is about reproducibility, here you should describe the method presented in the original paper.

\section{Implementation}

% This section should be structured as follows (from the Reproducibility challenge template):

% ---

% Briefly describe what you did and which resources did you use. E.g. Did you use author's code, did you re-implement parts of the pipeline, how much time did it take to produce the results, what hardware you were using and how long it took to train/evaluate. 

\subsection{Datasets}
% Describe the datasets you used and how you obtained them. 

\subsection{Hyperparameters}
% Describe how you set the hyperparameters and what was the source for their value (e.g. paper, code or your guess). 

\subsection{Experimental setup}
% Explain how you ran your experiments, e.g. the CPU/GPU resources and provide the link to your code and notebooks.

\subsection{Computational requirements}
% Provide information on computational requirements for each of your experiments. For example, the number of CPU/GPU hours and memory requirements. You'll need to think about this ahead of time, and write your code in a way that captures this information so you can later add it to this section. 

\section{Results}

% In this section you should report the results of your work (e.g., the outcome of an empirical analysis). You should be objective and support your statements with empirical evidence.

% \begin{table}[h]
% \small\sf\centering
% \caption{Experimental results (average of 3 runs).}
% \begin{tabular}{l c c}
% \toprule
% Methods & MAE & MSE\\
% \midrule
% \texttt{Baseline1} & $21.23 \pm 1.65$ & $841.36 \pm 12.65$\\
% \texttt{Baseline2} & $15.45 \pm 1.02$ & $652.38 \pm 09.89$\\
% \midrule
% \texttt{Method} & $12.03 \pm 0.35$ & $324.13 \pm 05.56$\\
% \bottomrule
% \end{tabular}
% \label{tab:table}
% \end{table}

% Use figures, plots and tables (like \autoref{tab:table}) to present your results in a nice and readable way.

\section{Discussion and conclusion}

% Here you can express your judgments and draw your conclusions based on the  evidences produced on the previous sections.

% Try to summarize the achievements of your project and its limits, suggesting (when appropriate) possible extensions and future works.

% Bibliography
\bibliography{bibliography}
\bibliographystyle{unsrtnat}


% \clearpage

\end{document}
